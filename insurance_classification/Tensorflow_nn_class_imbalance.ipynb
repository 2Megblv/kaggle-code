{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A deep neural network for insurance classification written in tensorflow\n",
    "\n",
    "## Original goal, and a lesson learned\n",
    "\n",
    "I had originally set out to write this post with the intent of gaining practice in the design of neural networks using tensorflow, but along the way I learned a valuable lesson about class imbalance which I will share here in additon to the model I have designed. This process has shown me that you can design a really nifty model, but if it is given data it cannot effectively learn from, your predictions will be just as garbage as your inputs.\n",
    "\n",
    "Executable script versions of the different nn variants are avaliable at: https://github.com/CNuge/kaggle_code/tree/master/insurance_classification\n",
    "\n",
    "## What this post covers\n",
    "\n",
    "1. Designing the neural network\n",
    "2. My first training attempt (a.k.a. how to do it wrong)\n",
    "3. Why the training was not working\n",
    "4. Solution A: Downsampling the 0s\n",
    "5. Solution B: Upsampling the 1s\n",
    "6. Discussion of the results from the two methods of addressing class imbalance\n",
    "\n",
    "So If all you want is the best form of the working model, then you can look at just parts 1. and 5. If you're interested in learning from my mistakes in dealing with class imbalance then read on!\n",
    "\n",
    "### Housekeeping: imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danzferg/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Designing the network\n",
    "\n",
    "The first three functions below are not part of the network. The two gini functions are used to assess the normalized gini index score for the model, and I will be calling them once per training epoch so that we can check in on the model's accuracy.\n",
    "\n",
    "### 1.a gini assessment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "\tassert( len(actual) == len(pred) )\n",
    "\tall = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "\tall = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "\ttotalLosses = all[:,0].sum()\n",
    "\tginiSum = all[:,0].cumsum().sum() / totalLosses\n",
    "\n",
    "\tginiSum -= (len(actual) + 1) / 2.\n",
    "\treturn giniSum / len(actual)\n",
    " \n",
    "def gini_normalized(a, p):\n",
    "\treturn gini(a, p) / gini(a, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b reset function for the tensorflow graph\n",
    "Since we will be running multiple models in this notebook, we need to reset the tensorflow graph between runs so that the various parts aren't erroneously linked together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#for stability\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c Load the data, split categoricals, clean the data and standardize scale\n",
    "\n",
    "To keep things chronological, this is how I first went about importing the data. Note that I do not even look at the number of 0s and the number of 1s in the training data! This mistake would come to bite me in the butt and cause the training to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "test_dat = pd.read_csv('test.csv')\n",
    "train_dat = pd.read_csv('train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "train_y = train_dat['target'].as_matrix()\n",
    "train_x = train_dat.drop(['target', 'id'], axis = 1)\n",
    "test_dat = test_dat.drop(['id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean the data\n",
    "\n",
    "merged_dat = pd.concat([train_x, test_dat],axis=0)\n",
    "\n",
    "cat_features = [col for col in merged_dat.columns if col.endswith('cat')]\n",
    "for column in cat_features:\n",
    "\ttemp=pd.get_dummies(pd.Series(merged_dat[column]))\n",
    "\tmerged_dat=pd.concat([merged_dat,temp],axis=1)\n",
    "\tmerged_dat=merged_dat.drop([column],axis=1)\n",
    "\n",
    "numeric_features = [col for col in merged_dat.columns if '_calc_' in  str(col)]\n",
    "numeric_features = [col for col in numeric_features if '_bin' not in str(col)]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_numerics = scaler.fit_transform(merged_dat[numeric_features])\n",
    "scaled_num_df = pd.DataFrame(scaled_numerics, columns =numeric_features )\n",
    "\n",
    "\n",
    "merged_dat = merged_dat.drop(numeric_features, axis=1)\n",
    "\n",
    "\n",
    "merged_dat = np.concatenate((merged_dat.values,scaled_num_df), axis = 1)\n",
    "\n",
    "\n",
    "train_x = merged_dat[:train_x.shape[0]]\n",
    "test_dat = merged_dat[train_x.shape[0]:]\n",
    "\n",
    "\n",
    "train_x = train_x.astype(np.float32)\n",
    "test_dat = test_dat.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.d Designing the deep neural network\n",
    "\n",
    "To make the network I've used the tf.layers API which lets you define the makeup in a given layer of the neural network very easily. Here, the input for any layer is always the variable name given to the previous layer. The second thing passed in is the number of neurons, followed by some hyperparamater arguments (discussed below).\n",
    "\n",
    "Notes on the components I've used:\n",
    "- This neural network is designed using 4 fully connected hidden layers, a batch normalization layer and an output layer with a sigmoid activation function (so that useful probability predictions are generated). \n",
    "- The batch normalization applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1 (effectively centering the outputs of the previous layer). \n",
    "- Throughout the network the rectified linear unit (ReLU) activation function(https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) is used, along with an He Kernel initializer (https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_inputs = train_x.shape[1]\n",
    "learning_rate = 0.1\n",
    "num_classes = 2\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 400\n",
    "n_hidden3 = 200\n",
    "n_hidden4 = 100\n",
    "dropout = 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, num_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "\n",
    "with tf.variable_scope('ClassNet'):\n",
    "\n",
    "\the_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "\ttraining = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "\thidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "\t\t\t\t\t\t\t  kernel_initializer=he_init, name=\"hidden1\")\n",
    "\n",
    "\tbn1 = tf.layers.batch_normalization(hidden1, training = training, momentum = 0.9)\n",
    "\n",
    "\thidden2 = tf.layers.dense(bn1, n_hidden2, activation=tf.nn.relu,\n",
    "\t\t\t\t\t\t\t  kernel_initializer=he_init, name=\"hidden2\")\n",
    "\n",
    "\thidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu,\n",
    "\t\t\t\t\t\t\t  kernel_initializer=he_init, name=\"hidden3\")\n",
    "\n",
    "\thidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "\t\t\t\t\t\t\t  kernel_initializer=he_init, name=\"hidden4\")\n",
    "\n",
    "\tfc1 = tf.layers.dropout(hidden4, rate=dropout)\n",
    "\n",
    "\tlogits = tf.layers.dense(fc1, num_classes, activation=tf.nn.sigmoid)\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the neural network graph defined, the next this we need to do is define the methods used to calculate loss, to train the model, and to evaluate the model. These are all shown below. With all of the scopes defined we can initialize the network and supporting tf functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "\txentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "\tloss = tf.reduce_mean(xentropy, name=\"loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "\toptimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\ttraining_op = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "\tcorrect = tf.nn.in_top_k(logits, y, 1)\n",
    "\taccuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "#saver = tf.train.Saver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The first attempt at training the network\n",
    "\n",
    "\n",
    "I've defined n_epochs = 20 for brevity's sake, as on further iterations the flatline in the GINI NORM score persists.\n",
    "As you can see I here just pass all the training data into the model and leave it to run, note the scores and how they are changing from epoch to epoch, we aren't getting significant improvements to the final score!\n",
    "\n",
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.963649 Test accuracy: 0.963165 \n",
      "GINI NORM: 0.0341781845183\n",
      "1 Train accuracy: 0.963649 Test accuracy: 0.963165 \n",
      "GINI NORM: 0.0347636388603\n",
      "2 Train accuracy: 0.963649 Test accuracy: 0.963165 \n",
      "GINI NORM: 0.035081418971\n",
      "3 Train accuracy: 0.963649 Test accuracy: 0.963165 \n",
      "GINI NORM: 0.0352921015436\n",
      "4 Train accuracy: 0.963649 Test accuracy: 0.963165 \n",
      "GINI NORM: 0.0354550570331\n",
      "5 Train accuracy: 0.963649 Test accuracy: 0.963165 \n",
      "GINI NORM: 0.0355667530465\n",
      "6 Train accuracy: 0.963649 Test accuracy: 0.963165 \n",
      "GINI NORM: 0.0356500745572\n",
      "7 Train accuracy: 0.963649 Test accuracy: 0.963165 \n",
      "GINI NORM: 0.0356891138632\n",
      "8 Train accuracy: 0.963649 Test accuracy: 0.963165 \n",
      "GINI NORM: 0.0357117442497\n",
      "9 Train accuracy: 0.963649 Test accuracy: 0.963165 \n",
      "GINI NORM: 0.0357168797443\n",
      "10 Train accuracy: 0.963649 Test accuracy: 0.963165 \n",
      "GINI NORM: 0.035721116229\n",
      "11 Train accuracy: 0.963649 Test accuracy: 0.963165 \n",
      "GINI NORM: 0.0357215776677\n",
      "12 Train accuracy: 0.963649 Test accuracy: 0.963165 \n",
      "GINI NORM: 0.0357096319742\n",
      "13 Train accuracy: 0.963649 Test accuracy: 0.963165 \n",
      "GINI NORM: 0.0356976504794\n",
      "14 Train accuracy: 0.963649 Test accuracy: 0.963165 \n",
      "GINI NORM: 0.0356817706231\n",
      "15 Train accuracy: 0.963649 Test accuracy: 0.963165 \n",
      "GINI NORM: 0.0356589055394\n",
      "16 Train accuracy: 0.963649 Test accuracy: 0.963165 \n",
      "GINI NORM: 0.035630908939\n",
      "17 Train accuracy: 0.963649 Test accuracy: 0.963165 \n",
      "GINI NORM: 0.0355986639201\n",
      "18 Train accuracy: 0.963649 Test accuracy: 0.963165 \n",
      "GINI NORM: 0.0355669758101\n",
      "19 Train accuracy: 0.963649 Test accuracy: 0.963165 \n",
      "GINI NORM: 0.0355376824078\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_epochs = 20\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\tinit.run()\n",
    "\tfor epoch in range(n_epochs):\n",
    "\t\tsess.run(training_op, feed_dict={X: X_train, y: y_train})\t\n",
    "\t\tacc_train = accuracy.eval(feed_dict={X: X_train, y: y_train})\n",
    "\t\tacc_test = accuracy.eval(feed_dict={X: X_val,\n",
    "\t\t\t\t\t\t\t\t\t\t\ty: y_val})\n",
    "\n",
    "\t\t###below is the new GINI test.\n",
    "\t\tprob_test = logits.eval(feed_dict={X: X_val,\n",
    "\t\t\t\t\t\t\t\ty: y_val})\n",
    "\t\t#switched from outputs to logits\n",
    "\t\tgini_n = gini_normalized(y_val, prob_test[:,1])\n",
    "\n",
    "\t\tprint(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test, \n",
    "\t\t\t\"\\nGINI NORM:\", gini_n)\n",
    "\t\n",
    "\t#save_path = saver.save(sess, \"./cams_model_final.ckpt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Why is this not working ?!?\n",
    "\n",
    "\n",
    "Without even evaluating on the test data we can see that this is not working! The GINI NORM scores flounder around 0.03 and considering that the leader board has scores in the 0.289 range recorded this tells us that it is way way off base! So why is the accuracy improving but the gini score flatlined? Well I asked this same question (https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/43282) and was politely informed in the discussion section that the issue is that I have completely forgotten to consider the distribution of the two classes in the data! Which is a bonehead move on my part.  96%+ of the training data is 0s.... so by just predicting a 0 each time I would be correct 96% of the time. This is no good because we cannot get an informative probability, and the order of the data relative to one another (which is what matters in the gini score) is all jumbled. For this reason accuracy isn't what we should be focusing on, this is more of a ranking task than a classification one. \n",
    "\n",
    "Rodrigo Bicalho provided the a comment with a suggestion that I focused on as the best way to resolve the issue: 'There is a couple things you can try: i) Oversampling or Undersampling your sample If you do this, in your training set you might get 50%/50% split between Y=1 and Y=0. That way, your model will be less driven to predict all zeros.'\n",
    "\n",
    "\n",
    "So by giving a higher proportion of 1s relative to the number of 0s, then the model can begin to learn the features that are informative in distinguishing 'claim' from 'no claim'. We need to balance the class distribution in the dataset and can do this through either of the two methods. Undersampling is dropping instances from the class in the higher proportion in order to restore an even ratio. Oversampling is the opposite, where we duplicate the observations of the class in the lower proportion in order to bring the two classes into an even ratio.\n",
    "\n",
    "But which one is better, over or under sampling? There is an obvious speed advantage to undersampling as the size of the training set is decreased significantly. But this comes at a cost of throwing out lots of really useful data, so maybe it is better to increase the number of 1s and work with an expanded training set with a 50:50 ratio of 1s and 0s. I decided to try both and compare the results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Solution A: Downsampling the 0s\n",
    "\n",
    "This method scores: ~0.246 with epoch change below.\n",
    "\n",
    "In order to balance the 0s and 1s through downsampling, I split the 0s and 1s into separate dataframes, then I take a random sample of the 0s dataframe that is equal to the size of the the 1s dataframe. This provides a 50:50 split of the 0s and 1s to pass in to the neural network. After training, the model scores ~0.246 on the public leaderboard which is much higher then the dud of a GINI score that we were getting from the unaltered training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dat = pd.read_csv('test.csv')\n",
    "train_dat = pd.read_csv('train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "train_dat_1s = train_dat[train_dat['target'] == 1]\n",
    "\n",
    "train_dat_0s = train_dat[train_dat['target'] == 0]\n",
    "keep_0s = train_dat_0s.sample(frac=train_dat_1s.shape[0]/train_dat_0s.shape[0])\n",
    "\n",
    "\n",
    "train_dat = pd.concat([keep_0s,train_dat_1s],axis=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning steps - unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_y = train_dat['target'].as_matrix()\n",
    "train_x = train_dat.drop(['target', 'id'], axis = 1)\n",
    "test_dat = test_dat.drop(['id'], axis = 1)\n",
    "\n",
    "merged_dat = pd.concat([train_x, test_dat],axis=0)\n",
    "\n",
    "cat_features = [col for col in merged_dat.columns if col.endswith('cat')]\n",
    "for column in cat_features:\n",
    "\ttemp=pd.get_dummies(pd.Series(merged_dat[column]))\n",
    "\tmerged_dat=pd.concat([merged_dat,temp],axis=1)\n",
    "\tmerged_dat=merged_dat.drop([column],axis=1)\n",
    "\n",
    "numeric_features = [col for col in merged_dat.columns if '_calc_' in  str(col)]\n",
    "numeric_features = [col for col in numeric_features if '_bin' not in str(col)]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_numerics = scaler.fit_transform(merged_dat[numeric_features])\n",
    "scaled_num_df = pd.DataFrame(scaled_numerics, columns =numeric_features )\n",
    "\n",
    "\n",
    "merged_dat = merged_dat.drop(numeric_features, axis=1)\n",
    "\n",
    "\n",
    "merged_dat = np.concatenate((merged_dat.values,scaled_num_df), axis = 1)\n",
    "\n",
    "\n",
    "train_x = merged_dat[:train_x.shape[0]]\n",
    "test_dat = merged_dat[train_x.shape[0]:]\n",
    "\n",
    "\n",
    "train_x = train_x.astype(np.float32)\n",
    "test_dat = test_dat.astype(np.float32)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "num_inputs = train_x.shape[1]\n",
    "learning_rate = 0.1\n",
    "num_classes = 2\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 400\n",
    "n_hidden3 = 200\n",
    "n_hidden4 = 100\n",
    "dropout = 0.3\n",
    "n_epochs = 1500\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, num_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "\n",
    "with tf.variable_scope('ClassNet'):\n",
    "\n",
    "\the_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "\ttraining = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "\thidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "\t\t\t\t\t\t\t  kernel_initializer=he_init, name=\"hidden1\")\n",
    "\n",
    "\tbn1 = tf.layers.batch_normalization(hidden1, training = training, momentum = 0.9)\n",
    "\n",
    "\thidden2 = tf.layers.dense(bn1, n_hidden2, activation=tf.nn.relu,\n",
    "\t\t\t\t\t\t\t  kernel_initializer=he_init, name=\"hidden2\")\n",
    "\n",
    "\thidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu,\n",
    "\t\t\t\t\t\t\t  kernel_initializer=he_init, name=\"hidden3\")\n",
    "\n",
    "\thidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "\t\t\t\t\t\t\t  kernel_initializer=he_init, name=\"hidden4\")\n",
    "\n",
    "\tfc1 = tf.layers.dropout(hidden4, rate=dropout)\n",
    "\n",
    "\tlogits = tf.layers.dense(fc1, num_classes, activation=tf.nn.sigmoid)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "\txentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "\tloss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "\toptimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\ttraining_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "\tcorrect = tf.nn.in_top_k(logits, y, 1)\n",
    "\taccuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=0.2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.501815 Test accuracy: 0.508297 \n",
      "GINI NORM: 0.0124141401215\n",
      "1 Train accuracy: 0.503198 Test accuracy: 0.505301 \n",
      "GINI NORM: 0.0174508953024\n",
      "2 Train accuracy: 0.507606 Test accuracy: 0.511062 \n",
      "GINI NORM: 0.0229180488473\n",
      "3 Train accuracy: 0.511582 Test accuracy: 0.515787 \n",
      "GINI NORM: 0.0279544852933\n",
      "4 Train accuracy: 0.51452 Test accuracy: 0.519014 \n",
      "GINI NORM: 0.0325937780503\n",
      "5 Train accuracy: 0.518669 Test accuracy: 0.52201 \n",
      "GINI NORM: 0.0369504591934\n",
      "6 Train accuracy: 0.524056 Test accuracy: 0.523162 \n",
      "GINI NORM: 0.0408667549456\n",
      "7 Train accuracy: 0.527859 Test accuracy: 0.525006 \n",
      "GINI NORM: 0.0445220068123\n",
      "8 Train accuracy: 0.530193 Test accuracy: 0.529039 \n",
      "GINI NORM: 0.0479483007738\n",
      "9 Train accuracy: 0.533593 Test accuracy: 0.531574 \n",
      "GINI NORM: 0.0511321437191\n",
      "10 Train accuracy: 0.534226 Test accuracy: 0.53434 \n",
      "GINI NORM: 0.0542067668376\n",
      "11 Train accuracy: 0.535811 Test accuracy: 0.536299 \n",
      "GINI NORM: 0.0570945050581\n",
      "12 Train accuracy: 0.537626 Test accuracy: 0.537566 \n",
      "GINI NORM: 0.0598219196224\n",
      "13 Train accuracy: 0.538807 Test accuracy: 0.539295 \n",
      "GINI NORM: 0.0624605133937\n",
      "14 Train accuracy: 0.540536 Test accuracy: 0.540217 \n",
      "GINI NORM: 0.0650784956414\n",
      "15 Train accuracy: 0.541256 Test accuracy: 0.541023 \n",
      "GINI NORM: 0.0675814145891\n",
      "16 Train accuracy: 0.542351 Test accuracy: 0.54206 \n",
      "GINI NORM: 0.0699838257974\n",
      "17 Train accuracy: 0.543071 Test accuracy: 0.543443 \n",
      "GINI NORM: 0.0722481185477\n",
      "18 Train accuracy: 0.543791 Test accuracy: 0.543904 \n",
      "GINI NORM: 0.0744276278139\n",
      "19 Train accuracy: 0.544454 Test accuracy: 0.544711 \n",
      "GINI NORM: 0.0764245019807\n",
      "20 Train accuracy: 0.544944 Test accuracy: 0.544135 \n",
      "GINI NORM: 0.0784157451642\n",
      "21 Train accuracy: 0.544771 Test accuracy: 0.54425 \n",
      "GINI NORM: 0.080392964012\n",
      "22 Train accuracy: 0.545232 Test accuracy: 0.545287 \n",
      "GINI NORM: 0.0822138965124\n",
      "23 Train accuracy: 0.54601 Test accuracy: 0.545633 \n",
      "GINI NORM: 0.0841239685407\n",
      "24 Train accuracy: 0.546528 Test accuracy: 0.547246 \n",
      "GINI NORM: 0.0859012343593\n",
      "25 Train accuracy: 0.547306 Test accuracy: 0.547246 \n",
      "GINI NORM: 0.0876452455032\n",
      "26 Train accuracy: 0.548027 Test accuracy: 0.547707 \n",
      "GINI NORM: 0.0893075542669\n",
      "27 Train accuracy: 0.549006 Test accuracy: 0.547822 \n",
      "GINI NORM: 0.0909270463086\n",
      "28 Train accuracy: 0.549928 Test accuracy: 0.548168 \n",
      "GINI NORM: 0.0925310265851\n",
      "29 Train accuracy: 0.550763 Test accuracy: 0.549205 \n",
      "GINI NORM: 0.0940817781328\n",
      "30 Train accuracy: 0.551109 Test accuracy: 0.549666 \n",
      "GINI NORM: 0.0955734574785\n",
      "31 Train accuracy: 0.551714 Test accuracy: 0.550012 \n",
      "GINI NORM: 0.097012970545\n",
      "32 Train accuracy: 0.552463 Test accuracy: 0.551279 \n",
      "GINI NORM: 0.0984590707996\n",
      "33 Train accuracy: 0.552867 Test accuracy: 0.55174 \n",
      "GINI NORM: 0.099856404614\n",
      "34 Train accuracy: 0.553011 Test accuracy: 0.551855 \n",
      "GINI NORM: 0.101221333713\n",
      "35 Train accuracy: 0.553241 Test accuracy: 0.551855 \n",
      "GINI NORM: 0.102577338235\n",
      "36 Train accuracy: 0.553673 Test accuracy: 0.552316 \n",
      "GINI NORM: 0.103921655811\n",
      "37 Train accuracy: 0.553875 Test accuracy: 0.553814 \n",
      "GINI NORM: 0.105219544336\n",
      "38 Train accuracy: 0.554653 Test accuracy: 0.554275 \n",
      "GINI NORM: 0.106529332297\n",
      "39 Train accuracy: 0.555488 Test accuracy: 0.555427 \n",
      "GINI NORM: 0.107790566307\n",
      "40 Train accuracy: 0.555719 Test accuracy: 0.555312 \n",
      "GINI NORM: 0.109028638915\n",
      "41 Train accuracy: 0.555661 Test accuracy: 0.555312 \n",
      "GINI NORM: 0.110216670144\n",
      "42 Train accuracy: 0.555978 Test accuracy: 0.555658 \n",
      "GINI NORM: 0.111419575667\n",
      "43 Train accuracy: 0.556151 Test accuracy: 0.556465 \n",
      "GINI NORM: 0.112550765838\n",
      "44 Train accuracy: 0.556122 Test accuracy: 0.55658 \n",
      "GINI NORM: 0.113678343679\n",
      "45 Train accuracy: 0.556641 Test accuracy: 0.557271 \n",
      "GINI NORM: 0.114818883407\n",
      "46 Train accuracy: 0.556756 Test accuracy: 0.557732 \n",
      "GINI NORM: 0.11594157398\n",
      "47 Train accuracy: 0.557419 Test accuracy: 0.558078 \n",
      "GINI NORM: 0.117006786025\n",
      "48 Train accuracy: 0.557476 Test accuracy: 0.55658 \n",
      "GINI NORM: 0.118151894287\n",
      "49 Train accuracy: 0.55788 Test accuracy: 0.55681 \n",
      "GINI NORM: 0.119199257178\n",
      "50 Train accuracy: 0.558168 Test accuracy: 0.556234 \n",
      "GINI NORM: 0.12024067035\n",
      "51 Train accuracy: 0.558081 Test accuracy: 0.556004 \n",
      "GINI NORM: 0.121242772885\n",
      "52 Train accuracy: 0.558168 Test accuracy: 0.555773 \n",
      "GINI NORM: 0.122233507209\n",
      "53 Train accuracy: 0.558629 Test accuracy: 0.555658 \n",
      "GINI NORM: 0.123169631618\n",
      "54 Train accuracy: 0.55883 Test accuracy: 0.555888 \n",
      "GINI NORM: 0.124063576776\n",
      "55 Train accuracy: 0.559262 Test accuracy: 0.555888 \n",
      "GINI NORM: 0.124948491112\n",
      "56 Train accuracy: 0.559407 Test accuracy: 0.55658 \n",
      "GINI NORM: 0.125815450048\n",
      "57 Train accuracy: 0.559493 Test accuracy: 0.556926 \n",
      "GINI NORM: 0.126653510352\n",
      "58 Train accuracy: 0.559522 Test accuracy: 0.556695 \n",
      "GINI NORM: 0.127512607161\n",
      "59 Train accuracy: 0.559896 Test accuracy: 0.556695 \n",
      "GINI NORM: 0.128364054331\n",
      "60 Train accuracy: 0.560012 Test accuracy: 0.55681 \n",
      "GINI NORM: 0.129195314958\n",
      "61 Train accuracy: 0.560703 Test accuracy: 0.557617 \n",
      "GINI NORM: 0.129994489605\n",
      "62 Train accuracy: 0.560761 Test accuracy: 0.557502 \n",
      "GINI NORM: 0.130837224688\n",
      "63 Train accuracy: 0.561077 Test accuracy: 0.557387 \n",
      "GINI NORM: 0.13165042367\n",
      "64 Train accuracy: 0.561423 Test accuracy: 0.557617 \n",
      "GINI NORM: 0.132464472612\n",
      "65 Train accuracy: 0.561423 Test accuracy: 0.557502 \n",
      "GINI NORM: 0.133277884085\n",
      "66 Train accuracy: 0.56151 Test accuracy: 0.557732 \n",
      "GINI NORM: 0.13409905144\n",
      "67 Train accuracy: 0.562086 Test accuracy: 0.558308 \n",
      "GINI NORM: 0.134907363153\n",
      "68 Train accuracy: 0.562086 Test accuracy: 0.558654 \n",
      "GINI NORM: 0.135714718663\n",
      "69 Train accuracy: 0.562201 Test accuracy: 0.55923 \n",
      "GINI NORM: 0.136496575379\n",
      "70 Train accuracy: 0.562748 Test accuracy: 0.558308 \n",
      "GINI NORM: 0.1372498522\n",
      "71 Train accuracy: 0.56295 Test accuracy: 0.558193 \n",
      "GINI NORM: 0.138019703235\n",
      "72 Train accuracy: 0.563411 Test accuracy: 0.559 \n",
      "GINI NORM: 0.138787110637\n",
      "73 Train accuracy: 0.563584 Test accuracy: 0.558424 \n",
      "GINI NORM: 0.139515526135\n",
      "74 Train accuracy: 0.563728 Test accuracy: 0.559115 \n",
      "GINI NORM: 0.140262534502\n",
      "75 Train accuracy: 0.564074 Test accuracy: 0.55923 \n",
      "GINI NORM: 0.140970763457\n",
      "76 Train accuracy: 0.564131 Test accuracy: 0.559345 \n",
      "GINI NORM: 0.141709803452\n",
      "77 Train accuracy: 0.564189 Test accuracy: 0.559461 \n",
      "GINI NORM: 0.14246966746\n",
      "78 Train accuracy: 0.563872 Test accuracy: 0.560383 \n",
      "GINI NORM: 0.143197126754\n",
      "79 Train accuracy: 0.563959 Test accuracy: 0.560152 \n",
      "GINI NORM: 0.143882194305\n",
      "80 Train accuracy: 0.564045 Test accuracy: 0.560728 \n",
      "GINI NORM: 0.144585535991\n",
      "81 Train accuracy: 0.56416 Test accuracy: 0.560267 \n",
      "GINI NORM: 0.145236392662\n",
      "82 Train accuracy: 0.564477 Test accuracy: 0.560844 \n",
      "GINI NORM: 0.145947383986\n",
      "83 Train accuracy: 0.564794 Test accuracy: 0.560959 \n",
      "GINI NORM: 0.146637976276\n",
      "84 Train accuracy: 0.56514 Test accuracy: 0.561304 \n",
      "GINI NORM: 0.147322193867\n",
      "85 Train accuracy: 0.565543 Test accuracy: 0.561535 \n",
      "GINI NORM: 0.147963594737\n",
      "86 Train accuracy: 0.565658 Test accuracy: 0.562226 \n",
      "GINI NORM: 0.148648768533\n",
      "87 Train accuracy: 0.565457 Test accuracy: 0.562457 \n",
      "GINI NORM: 0.149325761467\n",
      "88 Train accuracy: 0.565889 Test accuracy: 0.562457 \n",
      "GINI NORM: 0.14998968627\n",
      "89 Train accuracy: 0.566148 Test accuracy: 0.562572 \n",
      "GINI NORM: 0.150652442378\n",
      "90 Train accuracy: 0.566494 Test accuracy: 0.563379 \n",
      "GINI NORM: 0.151291505858\n",
      "91 Train accuracy: 0.566753 Test accuracy: 0.563955 \n",
      "GINI NORM: 0.151927806969\n",
      "92 Train accuracy: 0.566897 Test accuracy: 0.56407 \n",
      "GINI NORM: 0.152580151071\n",
      "93 Train accuracy: 0.566868 Test accuracy: 0.563494 \n",
      "GINI NORM: 0.153249919347\n",
      "94 Train accuracy: 0.566955 Test accuracy: 0.563263 \n",
      "GINI NORM: 0.153871771142\n",
      "95 Train accuracy: 0.567445 Test accuracy: 0.563148 \n",
      "GINI NORM: 0.154499997636\n",
      "96 Train accuracy: 0.567387 Test accuracy: 0.563263 \n",
      "GINI NORM: 0.155141186016\n",
      "97 Train accuracy: 0.5673 Test accuracy: 0.562802 \n",
      "GINI NORM: 0.155750075925\n",
      "98 Train accuracy: 0.5673 Test accuracy: 0.563263 \n",
      "GINI NORM: 0.156365021798\n",
      "99 Train accuracy: 0.567445 Test accuracy: 0.563609 \n",
      "GINI NORM: 0.157017259654\n",
      "100 Train accuracy: 0.56779 Test accuracy: 0.564531 \n",
      "GINI NORM: 0.157612762698\n",
      "101 Train accuracy: 0.568107 Test accuracy: 0.565107 \n",
      "GINI NORM: 0.158225796161\n",
      "102 Train accuracy: 0.568165 Test accuracy: 0.565568 \n",
      "GINI NORM: 0.158848497917\n",
      "103 Train accuracy: 0.568338 Test accuracy: 0.566029 \n",
      "GINI NORM: 0.159497548424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 Train accuracy: 0.56877 Test accuracy: 0.565914 \n",
      "GINI NORM: 0.160136080679\n",
      "105 Train accuracy: 0.568683 Test accuracy: 0.564992 \n",
      "GINI NORM: 0.160756657535\n",
      "106 Train accuracy: 0.568971 Test accuracy: 0.565683 \n",
      "GINI NORM: 0.161387752643\n",
      "107 Train accuracy: 0.569663 Test accuracy: 0.565222 \n",
      "GINI NORM: 0.161999617412\n",
      "108 Train accuracy: 0.569577 Test accuracy: 0.564877 \n",
      "GINI NORM: 0.162614350795\n",
      "109 Train accuracy: 0.569692 Test accuracy: 0.565107 \n",
      "GINI NORM: 0.163227384258\n",
      "110 Train accuracy: 0.569778 Test accuracy: 0.564877 \n",
      "GINI NORM: 0.163840736456\n",
      "111 Train accuracy: 0.569951 Test accuracy: 0.564646 \n",
      "GINI NORM: 0.164425508758\n",
      "112 Train accuracy: 0.570268 Test accuracy: 0.564761 \n",
      "GINI NORM: 0.165021224292\n",
      "113 Train accuracy: 0.570498 Test accuracy: 0.564416 \n",
      "GINI NORM: 0.165609502677\n",
      "114 Train accuracy: 0.570729 Test accuracy: 0.564877 \n",
      "GINI NORM: 0.166163676429\n",
      "115 Train accuracy: 0.570671 Test accuracy: 0.565107 \n",
      "GINI NORM: 0.166711475482\n",
      "116 Train accuracy: 0.571075 Test accuracy: 0.564877 \n",
      "GINI NORM: 0.167298478928\n",
      "117 Train accuracy: 0.571276 Test accuracy: 0.564531 \n",
      "GINI NORM: 0.167878257716\n",
      "118 Train accuracy: 0.571507 Test accuracy: 0.563955 \n",
      "GINI NORM: 0.168443374699\n",
      "119 Train accuracy: 0.571392 Test accuracy: 0.564301 \n",
      "GINI NORM: 0.168992448692\n",
      "120 Train accuracy: 0.571593 Test accuracy: 0.564185 \n",
      "GINI NORM: 0.169546622443\n",
      "121 Train accuracy: 0.571853 Test accuracy: 0.564185 \n",
      "GINI NORM: 0.170102389869\n",
      "122 Train accuracy: 0.572141 Test accuracy: 0.564416 \n",
      "GINI NORM: 0.170661450889\n",
      "123 Train accuracy: 0.572112 Test accuracy: 0.564646 \n",
      "GINI NORM: 0.171225611667\n",
      "124 Train accuracy: 0.572313 Test accuracy: 0.564531 \n",
      "GINI NORM: 0.171778510478\n",
      "125 Train accuracy: 0.5724 Test accuracy: 0.564992 \n",
      "GINI NORM: 0.172297198411\n",
      "126 Train accuracy: 0.572342 Test accuracy: 0.564992 \n",
      "GINI NORM: 0.172880802018\n",
      "127 Train accuracy: 0.572803 Test accuracy: 0.565453 \n",
      "GINI NORM: 0.173435506994\n",
      "128 Train accuracy: 0.573178 Test accuracy: 0.566144 \n",
      "GINI NORM: 0.173998180342\n",
      "129 Train accuracy: 0.573322 Test accuracy: 0.566605 \n",
      "GINI NORM: 0.174567547124\n",
      "130 Train accuracy: 0.573523 Test accuracy: 0.567297 \n",
      "GINI NORM: 0.175132239127\n",
      "131 Train accuracy: 0.573984 Test accuracy: 0.567181 \n",
      "GINI NORM: 0.175688325288\n",
      "132 Train accuracy: 0.574157 Test accuracy: 0.567181 \n",
      "GINI NORM: 0.176268210321\n",
      "133 Train accuracy: 0.574186 Test accuracy: 0.568103 \n",
      "GINI NORM: 0.176802941243\n",
      "134 Train accuracy: 0.574474 Test accuracy: 0.567873 \n",
      "GINI NORM: 0.177347234212\n",
      "135 Train accuracy: 0.574561 Test accuracy: 0.568334 \n",
      "GINI NORM: 0.177864009735\n",
      "136 Train accuracy: 0.57433 Test accuracy: 0.568218 \n",
      "GINI NORM: 0.178418502221\n",
      "137 Train accuracy: 0.574157 Test accuracy: 0.568679 \n",
      "GINI NORM: 0.178959501596\n",
      "138 Train accuracy: 0.574474 Test accuracy: 0.569025 \n",
      "GINI NORM: 0.179480739408\n",
      "139 Train accuracy: 0.574474 Test accuracy: 0.569601 \n",
      "GINI NORM: 0.180018338944\n",
      "140 Train accuracy: 0.574676 Test accuracy: 0.570638 \n",
      "GINI NORM: 0.180574850084\n",
      "141 Train accuracy: 0.574935 Test accuracy: 0.570754 \n",
      "GINI NORM: 0.18111606195\n",
      "142 Train accuracy: 0.574791 Test accuracy: 0.570408 \n",
      "GINI NORM: 0.18165695508\n",
      "143 Train accuracy: 0.575281 Test accuracy: 0.570177 \n",
      "GINI NORM: 0.182193279676\n",
      "144 Train accuracy: 0.575828 Test accuracy: 0.570408 \n",
      "GINI NORM: 0.182715154957\n",
      "145 Train accuracy: 0.575944 Test accuracy: 0.570523 \n",
      "GINI NORM: 0.183196232171\n",
      "146 Train accuracy: 0.576318 Test accuracy: 0.571445 \n",
      "GINI NORM: 0.183709607855\n",
      "147 Train accuracy: 0.576088 Test accuracy: 0.571445 \n",
      "GINI NORM: 0.184203009484\n",
      "148 Train accuracy: 0.576174 Test accuracy: 0.57133 \n",
      "GINI NORM: 0.184710541695\n",
      "149 Train accuracy: 0.576549 Test accuracy: 0.57156 \n",
      "GINI NORM: 0.185183544291\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 150 # change to 1500\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\tinit.run()\n",
    "\tfor epoch in range(n_epochs):\n",
    "\t\tsess.run(training_op, feed_dict={X: X_train, y: y_train})\t\n",
    "\t\tacc_train = accuracy.eval(feed_dict={X: X_train, y: y_train})\n",
    "\t\tacc_test = accuracy.eval(feed_dict={X: X_val,\n",
    "\t\t\t\t\t\t\t\t\t\t\ty: y_val})\n",
    "\n",
    "\t\t###below is the new GINI test.\n",
    "\t\tprob_test = logits.eval(feed_dict={X: X_val,\n",
    "\t\t\t\t\t\t\t\ty: y_val})\n",
    "\t\t#switched from outputs to logits\n",
    "\t\tgini_n = gini_normalized(y_val, prob_test[:,1])\n",
    "\n",
    "\t\tprint(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test, \n",
    "\t\t\t\"\\nGINI NORM:\", gini_n)\n",
    "\t\n",
    "\t#save_path = saver.save(sess, \"./cams_model_final.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./cams_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "#make external predictions on the test_dat\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./cams_model_final.ckpt\") # or better, use save_path\n",
    "    Z = logits.eval(feed_dict={X: test_dat}) #switched from outputs to logits\n",
    "    y_pred = Z[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dnn_output = submission\n",
    "dnn_output['target'] = y_pred\n",
    "\n",
    "dnn_output.to_csv('tf_dnn_downsample.csv', index=False, float_format='%.10f')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Solution B: Upsampling the 1s\n",
    "\n",
    "This method scores: ~0.248 with epoch change below.\n",
    "\n",
    "Here we do the inverse of the downsampling, instead of subsetting the 0s, we instead up the number of 1s. Below I use a list comprehension to duplicate (and then merge) a set of 26 copies of the 1s dataframe. This brings the instances of 0s and 1s up to a 50:50 ratio for the training of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dat = pd.read_csv('test.csv')\n",
    "train_dat = pd.read_csv('train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "\n",
    "train_dat_0s = train_dat[train_dat['target'] == 0]\n",
    "\n",
    "\n",
    "train_dat_1s = train_dat[train_dat['target'] == 1]\n",
    "rep_1 =[train_dat_1s for x in range(train_dat_0s.shape[0]//train_dat_1s.shape[0] )]\n",
    "keep_1s = pd.concat(rep_1, axis=0)\n",
    "\n",
    "train_dat = pd.concat([keep_1s,train_dat_0s],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning steps - unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_y = train_dat['target'].as_matrix()\n",
    "train_x = train_dat.drop(['target', 'id'], axis = 1)\n",
    "test_dat = test_dat.drop(['id'], axis = 1)\n",
    "\n",
    "merged_dat = pd.concat([train_x, test_dat],axis=0)\n",
    "\n",
    "cat_features = [col for col in merged_dat.columns if col.endswith('cat')]\n",
    "for column in cat_features:\n",
    "\ttemp=pd.get_dummies(pd.Series(merged_dat[column]))\n",
    "\tmerged_dat=pd.concat([merged_dat,temp],axis=1)\n",
    "\tmerged_dat=merged_dat.drop([column],axis=1)\n",
    "\n",
    "numeric_features = [col for col in merged_dat.columns if '_calc_' in  str(col)]\n",
    "numeric_features = [col for col in numeric_features if '_bin' not in str(col)]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_numerics = scaler.fit_transform(merged_dat[numeric_features])\n",
    "scaled_num_df = pd.DataFrame(scaled_numerics, columns =numeric_features )\n",
    "\n",
    "\n",
    "merged_dat = merged_dat.drop(numeric_features, axis=1)\n",
    "\n",
    "\n",
    "merged_dat = np.concatenate((merged_dat.values,scaled_num_df), axis = 1)\n",
    "\n",
    "\n",
    "train_x = merged_dat[:train_x.shape[0]]\n",
    "test_dat = merged_dat[train_x.shape[0]:]\n",
    "\n",
    "\n",
    "train_x = train_x.astype(np.float32)\n",
    "test_dat = test_dat.astype(np.float32)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "reset_graph()\n",
    "\n",
    "num_inputs = train_x.shape[1]\n",
    "learning_rate = 0.1\n",
    "num_classes = 2\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 400\n",
    "n_hidden3 = 200\n",
    "n_hidden4 = 100\n",
    "dropout = 0.3\n",
    "n_epochs = 1500\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, num_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "\n",
    "with tf.variable_scope('ClassNet'):\n",
    "\n",
    "\the_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "\ttraining = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "\thidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "\t\t\t\t\t\t\t  kernel_initializer=he_init, name=\"hidden1\")\n",
    "\n",
    "\tbn1 = tf.layers.batch_normalization(hidden1, training = training, momentum = 0.9)\n",
    "\n",
    "\thidden2 = tf.layers.dense(bn1, n_hidden2, activation=tf.nn.relu,\n",
    "\t\t\t\t\t\t\t  kernel_initializer=he_init, name=\"hidden2\")\n",
    "\n",
    "\thidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu,\n",
    "\t\t\t\t\t\t\t  kernel_initializer=he_init, name=\"hidden3\")\n",
    "\n",
    "\thidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "\t\t\t\t\t\t\t  kernel_initializer=he_init, name=\"hidden4\")\n",
    "\n",
    "\tfc1 = tf.layers.dropout(hidden4, rate=dropout)\n",
    "\n",
    "\tlogits = tf.layers.dense(fc1, num_classes, activation=tf.nn.sigmoid)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "\txentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "\tloss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "\toptimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\ttraining_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "\tcorrect = tf.nn.in_top_k(logits, y, 1)\n",
    "\taccuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.506862 Test accuracy: 0.508318 \n",
      "GINI NORM: 0.0252771371878\n",
      "1 Train accuracy: 0.507019 Test accuracy: 0.50881 \n",
      "GINI NORM: 0.0304947263132\n",
      "2 Train accuracy: 0.511443 Test accuracy: 0.51267 \n",
      "GINI NORM: 0.0360271011316\n",
      "3 Train accuracy: 0.514212 Test accuracy: 0.514784 \n",
      "GINI NORM: 0.0411865802769\n",
      "4 Train accuracy: 0.51786 Test accuracy: 0.518348 \n",
      "GINI NORM: 0.0459278058369\n",
      "5 Train accuracy: 0.519999 Test accuracy: 0.520168 \n",
      "GINI NORM: 0.0502453337927\n",
      "6 Train accuracy: 0.523073 Test accuracy: 0.523205 \n",
      "GINI NORM: 0.0541967047143\n",
      "7 Train accuracy: 0.525311 Test accuracy: 0.525253 \n",
      "GINI NORM: 0.057881112517\n",
      "8 Train accuracy: 0.527397 Test accuracy: 0.527324 \n",
      "GINI NORM: 0.0612745820019\n",
      "9 Train accuracy: 0.529203 Test accuracy: 0.528963 \n",
      "GINI NORM: 0.0644592833577\n",
      "10 Train accuracy: 0.531137 Test accuracy: 0.530932 \n",
      "GINI NORM: 0.0674451429313\n",
      "11 Train accuracy: 0.533357 Test accuracy: 0.533249 \n",
      "GINI NORM: 0.0702531319891\n",
      "12 Train accuracy: 0.534822 Test accuracy: 0.534752 \n",
      "GINI NORM: 0.0729120349585\n",
      "13 Train accuracy: 0.535956 Test accuracy: 0.53542 \n",
      "GINI NORM: 0.0754362466036\n",
      "14 Train accuracy: 0.537313 Test accuracy: 0.536712 \n",
      "GINI NORM: 0.0778367303892\n",
      "15 Train accuracy: 0.538982 Test accuracy: 0.538312 \n",
      "GINI NORM: 0.0801401889334\n",
      "16 Train accuracy: 0.539967 Test accuracy: 0.539262 \n",
      "GINI NORM: 0.0823384524981\n",
      "17 Train accuracy: 0.540972 Test accuracy: 0.540233 \n",
      "GINI NORM: 0.0844379709986\n",
      "18 Train accuracy: 0.541548 Test accuracy: 0.540795 \n",
      "GINI NORM: 0.0864555361622\n",
      "19 Train accuracy: 0.541644 Test accuracy: 0.540976 \n",
      "GINI NORM: 0.0884020040326\n",
      "20 Train accuracy: 0.542177 Test accuracy: 0.541367 \n",
      "GINI NORM: 0.0902741567621\n",
      "21 Train accuracy: 0.543017 Test accuracy: 0.542176 \n",
      "GINI NORM: 0.0920960486856\n",
      "22 Train accuracy: 0.543425 Test accuracy: 0.542782 \n",
      "GINI NORM: 0.0938566385933\n",
      "23 Train accuracy: 0.543726 Test accuracy: 0.543112 \n",
      "GINI NORM: 0.0955634816806\n",
      "24 Train accuracy: 0.544178 Test accuracy: 0.543437 \n",
      "GINI NORM: 0.0972183548041\n",
      "25 Train accuracy: 0.544592 Test accuracy: 0.5436 \n",
      "GINI NORM: 0.0988393246816\n",
      "26 Train accuracy: 0.544894 Test accuracy: 0.544026 \n",
      "GINI NORM: 0.100403432985\n",
      "27 Train accuracy: 0.545649 Test accuracy: 0.544756 \n",
      "GINI NORM: 0.101928671159\n",
      "28 Train accuracy: 0.546251 Test accuracy: 0.545252 \n",
      "GINI NORM: 0.103415152499\n",
      "29 Train accuracy: 0.546355 Test accuracy: 0.545204 \n",
      "GINI NORM: 0.104870166351\n",
      "30 Train accuracy: 0.546504 Test accuracy: 0.54534 \n",
      "GINI NORM: 0.106290856701\n",
      "31 Train accuracy: 0.546862 Test accuracy: 0.545604 \n",
      "GINI NORM: 0.107669081168\n",
      "32 Train accuracy: 0.547374 Test accuracy: 0.546105 \n",
      "GINI NORM: 0.109018056127\n",
      "33 Train accuracy: 0.547323 Test accuracy: 0.546246 \n",
      "GINI NORM: 0.110327816746\n",
      "34 Train accuracy: 0.547495 Test accuracy: 0.546378 \n",
      "GINI NORM: 0.111595428026\n",
      "35 Train accuracy: 0.548085 Test accuracy: 0.547182 \n",
      "GINI NORM: 0.11284024296\n",
      "36 Train accuracy: 0.548441 Test accuracy: 0.54752 \n",
      "GINI NORM: 0.114053404004\n",
      "37 Train accuracy: 0.548684 Test accuracy: 0.547793 \n",
      "GINI NORM: 0.115254825058\n",
      "38 Train accuracy: 0.549236 Test accuracy: 0.548285 \n",
      "GINI NORM: 0.116428041914\n",
      "39 Train accuracy: 0.549553 Test accuracy: 0.548619 \n",
      "GINI NORM: 0.117579863228\n",
      "40 Train accuracy: 0.54993 Test accuracy: 0.549147 \n",
      "GINI NORM: 0.118700291091\n",
      "41 Train accuracy: 0.550246 Test accuracy: 0.549384 \n",
      "GINI NORM: 0.119788461496\n",
      "42 Train accuracy: 0.550665 Test accuracy: 0.549933 \n",
      "GINI NORM: 0.120857055874\n",
      "43 Train accuracy: 0.550971 Test accuracy: 0.550245 \n",
      "GINI NORM: 0.121901424197\n",
      "44 Train accuracy: 0.551118 Test accuracy: 0.550531 \n",
      "GINI NORM: 0.122919073983\n",
      "45 Train accuracy: 0.551291 Test accuracy: 0.550755 \n",
      "GINI NORM: 0.123919391534\n",
      "46 Train accuracy: 0.551764 Test accuracy: 0.551296 \n",
      "GINI NORM: 0.124894380223\n",
      "47 Train accuracy: 0.552 Test accuracy: 0.551564 \n",
      "GINI NORM: 0.125857304032\n",
      "48 Train accuracy: 0.552485 Test accuracy: 0.552127 \n",
      "GINI NORM: 0.126814513645\n",
      "49 Train accuracy: 0.55261 Test accuracy: 0.552166 \n",
      "GINI NORM: 0.127757904859\n",
      "50 Train accuracy: 0.553137 Test accuracy: 0.552571 \n",
      "GINI NORM: 0.128680344128\n",
      "51 Train accuracy: 0.553501 Test accuracy: 0.552808 \n",
      "GINI NORM: 0.129590689458\n",
      "52 Train accuracy: 0.553841 Test accuracy: 0.55301 \n",
      "GINI NORM: 0.1304917862\n",
      "53 Train accuracy: 0.554201 Test accuracy: 0.553415 \n",
      "GINI NORM: 0.131378106716\n",
      "54 Train accuracy: 0.554537 Test accuracy: 0.553757 \n",
      "GINI NORM: 0.132260049698\n",
      "55 Train accuracy: 0.554661 Test accuracy: 0.553968 \n",
      "GINI NORM: 0.133129293627\n",
      "56 Train accuracy: 0.555124 Test accuracy: 0.554553 \n",
      "GINI NORM: 0.133986242995\n",
      "57 Train accuracy: 0.555491 Test accuracy: 0.555001 \n",
      "GINI NORM: 0.134838553926\n",
      "58 Train accuracy: 0.555606 Test accuracy: 0.555094 \n",
      "GINI NORM: 0.13568171658\n",
      "59 Train accuracy: 0.555967 Test accuracy: 0.555621 \n",
      "GINI NORM: 0.13653019914\n",
      "60 Train accuracy: 0.556041 Test accuracy: 0.555872 \n",
      "GINI NORM: 0.13736152072\n",
      "61 Train accuracy: 0.556634 Test accuracy: 0.556351 \n",
      "GINI NORM: 0.138184287237\n",
      "62 Train accuracy: 0.556995 Test accuracy: 0.556579 \n",
      "GINI NORM: 0.139000629806\n",
      "63 Train accuracy: 0.55709 Test accuracy: 0.556724 \n",
      "GINI NORM: 0.139806174137\n",
      "64 Train accuracy: 0.557491 Test accuracy: 0.55697 \n",
      "GINI NORM: 0.140603130174\n",
      "65 Train accuracy: 0.557807 Test accuracy: 0.557414 \n",
      "GINI NORM: 0.141389169579\n",
      "66 Train accuracy: 0.557973 Test accuracy: 0.557463 \n",
      "GINI NORM: 0.142170501457\n",
      "67 Train accuracy: 0.558123 Test accuracy: 0.557665 \n",
      "GINI NORM: 0.142944293287\n",
      "68 Train accuracy: 0.558316 Test accuracy: 0.557805 \n",
      "GINI NORM: 0.143705396266\n",
      "69 Train accuracy: 0.558528 Test accuracy: 0.558087 \n",
      "GINI NORM: 0.144461831904\n",
      "70 Train accuracy: 0.559069 Test accuracy: 0.558522 \n",
      "GINI NORM: 0.14520354155\n",
      "71 Train accuracy: 0.55932 Test accuracy: 0.558768 \n",
      "GINI NORM: 0.145943310503\n",
      "72 Train accuracy: 0.559447 Test accuracy: 0.559045 \n",
      "GINI NORM: 0.14667504125\n",
      "73 Train accuracy: 0.559636 Test accuracy: 0.559225 \n",
      "GINI NORM: 0.147398498857\n",
      "74 Train accuracy: 0.559664 Test accuracy: 0.559194 \n",
      "GINI NORM: 0.148114477314\n",
      "75 Train accuracy: 0.559935 Test accuracy: 0.559309 \n",
      "GINI NORM: 0.148825041427\n",
      "76 Train accuracy: 0.560121 Test accuracy: 0.559546 \n",
      "GINI NORM: 0.149525179491\n",
      "77 Train accuracy: 0.560516 Test accuracy: 0.559999 \n",
      "GINI NORM: 0.15021008676\n",
      "78 Train accuracy: 0.560554 Test accuracy: 0.55992 \n",
      "GINI NORM: 0.150889931626\n",
      "79 Train accuracy: 0.560819 Test accuracy: 0.560214 \n",
      "GINI NORM: 0.151572606542\n",
      "80 Train accuracy: 0.561219 Test accuracy: 0.560495 \n",
      "GINI NORM: 0.152245212757\n",
      "81 Train accuracy: 0.561741 Test accuracy: 0.5609 \n",
      "GINI NORM: 0.152906733867\n",
      "82 Train accuracy: 0.562003 Test accuracy: 0.561164 \n",
      "GINI NORM: 0.153561133331\n",
      "83 Train accuracy: 0.562191 Test accuracy: 0.561361 \n",
      "GINI NORM: 0.154215273283\n",
      "84 Train accuracy: 0.562204 Test accuracy: 0.56144 \n",
      "GINI NORM: 0.154866579786\n",
      "85 Train accuracy: 0.56244 Test accuracy: 0.561625 \n",
      "GINI NORM: 0.155509026118\n",
      "86 Train accuracy: 0.562708 Test accuracy: 0.561955 \n",
      "GINI NORM: 0.156143073804\n",
      "87 Train accuracy: 0.562691 Test accuracy: 0.561915 \n",
      "GINI NORM: 0.156771585042\n",
      "88 Train accuracy: 0.562907 Test accuracy: 0.562122 \n",
      "GINI NORM: 0.15739960276\n",
      "89 Train accuracy: 0.563002 Test accuracy: 0.56224 \n",
      "GINI NORM: 0.15802113455\n",
      "90 Train accuracy: 0.563302 Test accuracy: 0.562566 \n",
      "GINI NORM: 0.158637886479\n",
      "91 Train accuracy: 0.563378 Test accuracy: 0.562425 \n",
      "GINI NORM: 0.159245769889\n",
      "92 Train accuracy: 0.563493 Test accuracy: 0.56232 \n",
      "GINI NORM: 0.159850456473\n",
      "93 Train accuracy: 0.563716 Test accuracy: 0.562658 \n",
      "GINI NORM: 0.160451901097\n",
      "94 Train accuracy: 0.563766 Test accuracy: 0.562623 \n",
      "GINI NORM: 0.161059149409\n",
      "95 Train accuracy: 0.563998 Test accuracy: 0.562851 \n",
      "GINI NORM: 0.16166781089\n",
      "96 Train accuracy: 0.564081 Test accuracy: 0.563027 \n",
      "GINI NORM: 0.162270147034\n",
      "97 Train accuracy: 0.564261 Test accuracy: 0.563216 \n",
      "GINI NORM: 0.162869173983\n",
      "98 Train accuracy: 0.56463 Test accuracy: 0.563585 \n",
      "GINI NORM: 0.163459817124\n",
      "99 Train accuracy: 0.564647 Test accuracy: 0.563625 \n",
      "GINI NORM: 0.16404257106\n",
      "100 Train accuracy: 0.564752 Test accuracy: 0.563664 \n",
      "GINI NORM: 0.164622721535\n",
      "101 Train accuracy: 0.565043 Test accuracy: 0.563941 \n",
      "GINI NORM: 0.165194502268\n",
      "102 Train accuracy: 0.565339 Test accuracy: 0.564152 \n",
      "GINI NORM: 0.165766615776\n",
      "103 Train accuracy: 0.565412 Test accuracy: 0.564232 \n",
      "GINI NORM: 0.166330400654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 Train accuracy: 0.565733 Test accuracy: 0.564557 \n",
      "GINI NORM: 0.166892462\n",
      "105 Train accuracy: 0.565715 Test accuracy: 0.564636 \n",
      "GINI NORM: 0.167447170937\n",
      "106 Train accuracy: 0.565844 Test accuracy: 0.564794 \n",
      "GINI NORM: 0.168001424377\n",
      "107 Train accuracy: 0.565955 Test accuracy: 0.56475 \n",
      "GINI NORM: 0.168550475999\n",
      "108 Train accuracy: 0.566054 Test accuracy: 0.564728 \n",
      "GINI NORM: 0.169089646869\n",
      "109 Train accuracy: 0.56632 Test accuracy: 0.565001 \n",
      "GINI NORM: 0.169626376571\n",
      "110 Train accuracy: 0.56662 Test accuracy: 0.565225 \n",
      "GINI NORM: 0.170159124421\n",
      "111 Train accuracy: 0.566806 Test accuracy: 0.56548 \n",
      "GINI NORM: 0.170695860769\n",
      "112 Train accuracy: 0.567065 Test accuracy: 0.565827 \n",
      "GINI NORM: 0.171228466266\n",
      "113 Train accuracy: 0.567361 Test accuracy: 0.566038 \n",
      "GINI NORM: 0.171766487342\n",
      "114 Train accuracy: 0.567382 Test accuracy: 0.566082 \n",
      "GINI NORM: 0.172294988572\n",
      "115 Train accuracy: 0.567344 Test accuracy: 0.566051 \n",
      "GINI NORM: 0.172823008493\n",
      "116 Train accuracy: 0.567558 Test accuracy: 0.566218 \n",
      "GINI NORM: 0.173342588963\n",
      "117 Train accuracy: 0.567872 Test accuracy: 0.566486 \n",
      "GINI NORM: 0.173867828295\n",
      "118 Train accuracy: 0.56795 Test accuracy: 0.566539 \n",
      "GINI NORM: 0.174389303399\n",
      "119 Train accuracy: 0.568109 Test accuracy: 0.566741 \n",
      "GINI NORM: 0.174898327369\n",
      "120 Train accuracy: 0.568296 Test accuracy: 0.566979 \n",
      "GINI NORM: 0.175408897587\n",
      "121 Train accuracy: 0.568584 Test accuracy: 0.567387 \n",
      "GINI NORM: 0.17591419087\n",
      "122 Train accuracy: 0.568858 Test accuracy: 0.567594 \n",
      "GINI NORM: 0.176420697472\n",
      "123 Train accuracy: 0.568899 Test accuracy: 0.567752 \n",
      "GINI NORM: 0.176920264193\n",
      "124 Train accuracy: 0.569179 Test accuracy: 0.568038 \n",
      "GINI NORM: 0.177427805749\n",
      "125 Train accuracy: 0.569414 Test accuracy: 0.568205 \n",
      "GINI NORM: 0.17792654865\n",
      "126 Train accuracy: 0.569564 Test accuracy: 0.568526 \n",
      "GINI NORM: 0.178427075981\n",
      "127 Train accuracy: 0.569702 Test accuracy: 0.568737 \n",
      "GINI NORM: 0.1789199002\n",
      "128 Train accuracy: 0.569789 Test accuracy: 0.568926 \n",
      "GINI NORM: 0.179412206633\n",
      "129 Train accuracy: 0.569869 Test accuracy: 0.569084 \n",
      "GINI NORM: 0.179894241116\n",
      "130 Train accuracy: 0.5699 Test accuracy: 0.569088 \n",
      "GINI NORM: 0.180378182288\n",
      "131 Train accuracy: 0.569921 Test accuracy: 0.569053 \n",
      "GINI NORM: 0.180857165079\n",
      "132 Train accuracy: 0.570139 Test accuracy: 0.569207 \n",
      "GINI NORM: 0.181332107283\n",
      "133 Train accuracy: 0.570435 Test accuracy: 0.569484 \n",
      "GINI NORM: 0.181805621942\n",
      "134 Train accuracy: 0.570434 Test accuracy: 0.569497 \n",
      "GINI NORM: 0.182270507039\n",
      "135 Train accuracy: 0.570456 Test accuracy: 0.569739 \n",
      "GINI NORM: 0.182738992372\n",
      "136 Train accuracy: 0.570534 Test accuracy: 0.569875 \n",
      "GINI NORM: 0.183196775915\n",
      "137 Train accuracy: 0.570647 Test accuracy: 0.569941 \n",
      "GINI NORM: 0.183656631995\n",
      "138 Train accuracy: 0.57097 Test accuracy: 0.570244 \n",
      "GINI NORM: 0.184111441437\n",
      "139 Train accuracy: 0.571126 Test accuracy: 0.570367 \n",
      "GINI NORM: 0.184564210337\n",
      "140 Train accuracy: 0.571501 Test accuracy: 0.570706 \n",
      "GINI NORM: 0.185017163013\n",
      "141 Train accuracy: 0.571532 Test accuracy: 0.570816 \n",
      "GINI NORM: 0.185465742174\n",
      "142 Train accuracy: 0.57158 Test accuracy: 0.570948 \n",
      "GINI NORM: 0.185907119161\n",
      "143 Train accuracy: 0.571621 Test accuracy: 0.57093 \n",
      "GINI NORM: 0.186348622272\n",
      "144 Train accuracy: 0.57182 Test accuracy: 0.571194 \n",
      "GINI NORM: 0.186780306148\n",
      "145 Train accuracy: 0.571867 Test accuracy: 0.571137 \n",
      "GINI NORM: 0.187216141278\n",
      "146 Train accuracy: 0.572111 Test accuracy: 0.571479 \n",
      "GINI NORM: 0.18764928902\n",
      "147 Train accuracy: 0.572191 Test accuracy: 0.571624 \n",
      "GINI NORM: 0.188080387411\n",
      "148 Train accuracy: 0.572316 Test accuracy: 0.57173 \n",
      "GINI NORM: 0.188503754096\n",
      "149 Train accuracy: 0.572389 Test accuracy: 0.571682 \n",
      "GINI NORM: 0.188929179251\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 150 #change to 1500, we cut it off early here but you can see it rising.\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\tinit.run()\n",
    "\tfor epoch in range(n_epochs):\n",
    "\t\tsess.run(training_op, feed_dict={X: X_train, y: y_train})\t\n",
    "\t\tacc_train = accuracy.eval(feed_dict={X: X_train, y: y_train})\n",
    "\t\tacc_test = accuracy.eval(feed_dict={X: X_val,\n",
    "\t\t\t\t\t\t\t\t\t\t\ty: y_val})\n",
    "\n",
    "\t\t###below is the new GINI test.\n",
    "\t\tprob_test = logits.eval(feed_dict={X: X_val,\n",
    "\t\t\t\t\t\t\t\ty: y_val})\n",
    "\t\t#switched from outputs to logits\n",
    "\t\tgini_n = gini_normalized(y_val, prob_test[:,1])\n",
    "\n",
    "\t\tprint(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test, \n",
    "\t\t\t\"\\nGINI NORM:\", gini_n)\n",
    "\t\n",
    "\tsave_path = saver.save(sess, \"./cams_model_final.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./cams_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "#make external predictions on the test_dat\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./cams_model_final.ckpt\") # or better, use save_path\n",
    "    Z = logits.eval(feed_dict={X: test_dat}) #switched from outputs to logits\n",
    "    y_pred = Z[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dnn_output = submission\n",
    "dnn_output['target'] = y_pred\n",
    "\n",
    "dnn_output.to_csv('tf_dnn_predictions_upsample.csv', index=False, float_format='%.10f')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "So before any hyperparamater tuning, by simply creating balanced training data from a very imbalanced training set we are able to produce a model that is fairly effective at predicting the relative likelihoods of people filing insurance claims. This has taught me the importance of assessing the data I am using, and thinking of instances beyond the obvious things such as missing data and the need to one hot encode categoricals. \n",
    "\n",
    "The difference here between a model that is completely useless and one that is a good predictor is very small, just 3 lines of pandas dataframe manipulation in the preprocessing, but it proved to make all the difference. Over sampling and undersampling proved to be roughly equivalent, with a slightly higher LB score observed for the oversampled data. I have learned my lesson and will always check the class distribution in future classification problems that I undertake!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
