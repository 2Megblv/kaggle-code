{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Tensorflow based Convolutional Neural Network for Iceberg Classification\n",
    "\n",
    "In an attempt to improve my understanding of how to best design and use neural networks, I set out to build a convolutional neural network using just tensorflow (as opposed to my usual workflow of using nifty wrappers like Keras to make life easier). I am sharing the code here in the hopes that other people can learn a bit and maybe be tempted into experiment with using tensorflow themselves!\n",
    "\n",
    "NOTE: This solution abandons the non-image data in the train and test json files so it is definitely far from an optimal solution. This network is not designed to win the competition, but rather with the idea of me learning more about neural network design through the use of base tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Relitavely short list of libraries and imports, making use of pandas, numpy, the train/test split function from SciKit-Learn and obviously tensorflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danzferg/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations\n",
    "\n",
    "Here is a list of the resorces that went in to helping me design this convolutional neural network:\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/layers\n",
    "\n",
    "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network.py\n",
    "\n",
    "This example is using TensorFlow layers API\n",
    "TensorFlowâ€™s high-level machine learning API (tf.estimator) makes it easy to configure, \n",
    "train, and evaluate a variety of machine learning models:\n",
    "\n",
    "https://www.tensorflow.org/get_started/estimator\n",
    "\n",
    "I also read this book. It was neat.\n",
    "\n",
    "http://shop.oreilly.com/product/0636920052289.do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data, split the training data into a training and validation set\n",
    "\n",
    "Before the model is built there is the same old basic housekeeping of loading in the data and splitting off a validation set. As I mentioned above I am discarding everything except for the images and the labels... which is most definitely a loss of useful information. I am also not augmenting the data here (as I want to deal with one thing at a time. see this kernel for how I generated more image instances for training: https://www.kaggle.com/camnugent/expanded-training-set-keras-imagedatagenerator)\n",
    "\n",
    "Thank you to Kevin Mader, I have appropriated your input function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "Train (1122, 75, 75, 2) (1122,)\n",
      "Validation (482, 75, 75, 2) (482,)\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "# Load in the data\n",
    "#####\n",
    "print('loading data')\n",
    "# load function from: https://www.kaggle.com/kmader/exploring-the-icebergs-with-skimage-and-keras\n",
    "# b/c I didn't want to reinvent the wheel\n",
    "def load_and_format(in_path):\n",
    "    \"\"\" take the input data in .json format and return a df with the data and an np.array for the pictures \"\"\"\n",
    "    out_df = pd.read_json(in_path)\n",
    "    out_images = out_df.apply(lambda c_row: [np.stack([c_row['band_1'],c_row['band_2']], -1).reshape((75,75,2))],1)\n",
    "    out_images = np.stack(out_images).squeeze()\n",
    "    return out_df, out_images\n",
    "\n",
    "\n",
    "train_df, train_images = load_and_format('train.json')\n",
    "\n",
    "test_df, test_images = load_and_format('test.json')\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_images,\n",
    "                                                   train_df['is_iceberg'].as_matrix(),\n",
    "                                                   test_size = 0.3\n",
    "                                                   )\n",
    "print('Train', X_train.shape, y_train.shape)\n",
    "print('Validation', X_valid.shape, y_valid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data to float32 \n",
    "\n",
    "Tensorflow likes its data to be in float32, if you skip this step and pass in float64 values... it will yell at you. I don't like being yelled at so I avoid this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#convert to np.float32 for use in tensorflow\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "X_valid = X_valid.astype(np.float32)\n",
    "y_valid= y_valid.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a reset function\n",
    "\n",
    "This is here for iterative design purposes. If you define the neural network and don't do exactly how you want, then try to do it again without resetting the graph, then funny things can happen as tensorflow will try to patch the new onto the old. We must therefore always throw away the old!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#for stability\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set necessary paramaters/hyperparamaters\n",
    "\n",
    "For this model I'm using a slow learning rate (0.005) and a high number of epochs (2500).\n",
    "\n",
    "The input to the network is the length * width of the image (# of pixels).\n",
    "The dropout is used to prevent overfitting by randomly dropping components of neural network. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "designing model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('designing model')\n",
    "# Training Parameters\n",
    "learning_rate = 0.005\n",
    "n_epochs = 50 #move up to ~1500\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 75*75 #size of the images\n",
    "num_classes = 2 # Binary\n",
    "dropout = 0.3 # Dropout, probability to keep units\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design the convolutional neural network\n",
    "\n",
    "Here we get to the design of the network, first set is to design the graph in tensorflow. The variables X and y below are placeholders for the actual data we will pass in to the network. Note the shape of X is (None, 75, 75, 2). The None is so that the # of rows is flexiable, the 75,75 is the pixel dimensions of the image and the 2 is because there are two channels of image data being passed in. Note y has shape=(None) because it will be a 1-D vector with one input for each row. If we had multiple classes this could be changed to shape=(None, 5) (for 5 classes).\n",
    "\n",
    "The network used here uses an initial set of convolutional layers, followed by a pooling step and several additional fully connected layers. The second to last layer applies dropout, which we defined as 0.3. Throughout the network the rectified linear unit (ReLU) activation function(https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) is used, along with an He Kernel initializer (https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf).\n",
    "The Sigmoid activation function is important for the final layer as this lets us get meaningful probabilities returned from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 75, 75, 2), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "\n",
    "with tf.variable_scope('ConvNet'):\n",
    "\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    # Convolution Layer with 32 filters and a kernel size of 5\n",
    "    conv1 = tf.layers.conv2d(X, filters=32,  kernel_size=[5, 5], activation=tf.nn.relu)\n",
    "    # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "    pool1 = tf.layers.max_pooling2d(conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    conv2 = tf.layers.conv2d(pool1, filters=64,  kernel_size=[3,3], activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    conv3 = tf.layers.conv2d(pool2, filters=128, kernel_size=[3,3], activation=tf.nn.relu)\n",
    "    pool3 = tf.layers.max_pooling2d(conv3, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    conv4 = tf.layers.conv2d(pool3, filters=256, kernel_size=[3,3], activation=tf.nn.relu)\n",
    "    pool4 = tf.layers.max_pooling2d(conv4, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    # Flatten the data to a 1-D vector for the fully connected layer\n",
    "    fc1 = tf.contrib.layers.flatten(pool4)\n",
    "\n",
    "    # Fully connected layer (in tf contrib folder for now)\n",
    "    fc2 = tf.layers.dense(fc1, 32, \n",
    "                        kernel_initializer=he_init, activation=tf.nn.relu)\n",
    "\n",
    "    fc3 = tf.layers.dense(fc2, 128, \n",
    "                        kernel_initializer=he_init, activation=tf.nn.relu)\n",
    "\n",
    "    fc4 = tf.layers.dense(fc3, 512, \n",
    "                        kernel_initializer=he_init, activation=tf.nn.relu)\n",
    "\n",
    "    fc5 = tf.layers.dense(fc4, 32, \n",
    "                        kernel_initializer=he_init, activation=tf.nn.relu)\n",
    "\n",
    "\n",
    "    # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "    fc6 = tf.layers.dropout(fc5, rate=dropout)\n",
    "\n",
    "    logits = tf.layers.dense(fc6, num_classes, activation=tf.nn.sigmoid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "\n",
    "With the network defined we next define the loss function which compares the predicted values to the actual values of the training set. The sparse_softmax_cross_entropy_with_logits function used here computes the sparse softmax cross entropy between logits and labels and the reduce_mean function is then used to compute the mean of the tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training method\n",
    "\n",
    "Gradient descent is defined as the training method used to minimize the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the evalutation method\n",
    "\n",
    "This explains the evaluation method better then I can, so have a look if you're curious about how tf.nn.in_top_k() works!\n",
    "https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we initialize the network.\n",
    "I've commented out the saver portion, I use this when running the network locally to maintain a copy of the model after training (so we don't have to start from scratch each time). The saver, and corresponding use lines below are turned off because there is no need to save the model to memory when I run on Kaggle (you can turn them on though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "init = tf.global_variables_initializer()\n",
    "#saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Recall the number of epochs was defined above as 2500, so the model will be trained on the entire training set for 2500 iterations. Here I have it print the training and testing accuracy after each epoch.\n",
    "\n",
    "Here we initiate the model and for each epoch we use sess.run() to pass the data into the model and train the network. Predictions for the train and validation data are then made and the accuracy is assessed and printed to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model\n",
      "\n",
      "0 Train accuracy: 0.532977 Validation accuracy: 0.524896\n",
      "1 Train accuracy: 0.532977 Validation accuracy: 0.524896\n",
      "2 Train accuracy: 0.532977 Validation accuracy: 0.524896\n",
      "3 Train accuracy: 0.532977 Validation accuracy: 0.524896\n",
      "4 Train accuracy: 0.532977 Validation accuracy: 0.524896\n",
      "5 Train accuracy: 0.532977 Validation accuracy: 0.524896\n",
      "6 Train accuracy: 0.532977 Validation accuracy: 0.524896\n",
      "7 Train accuracy: 0.532977 Validation accuracy: 0.524896\n",
      "8 Train accuracy: 0.532977 Validation accuracy: 0.524896\n",
      "9 Train accuracy: 0.532977 Validation accuracy: 0.524896\n",
      "10 Train accuracy: 0.532977 Validation accuracy: 0.524896\n",
      "11 Train accuracy: 0.627451 Validation accuracy: 0.622407\n",
      "12 Train accuracy: 0.532977 Validation accuracy: 0.524896\n",
      "13 Train accuracy: 0.543672 Validation accuracy: 0.53112\n",
      "14 Train accuracy: 0.532977 Validation accuracy: 0.524896\n",
      "15 Train accuracy: 0.639929 Validation accuracy: 0.643154\n",
      "16 Train accuracy: 0.532977 Validation accuracy: 0.524896\n",
      "17 Train accuracy: 0.661319 Validation accuracy: 0.651452\n",
      "18 Train accuracy: 0.532977 Validation accuracy: 0.524896\n",
      "19 Train accuracy: 0.561497 Validation accuracy: 0.576764\n",
      "20 Train accuracy: 0.54902 Validation accuracy: 0.522822\n",
      "21 Train accuracy: 0.56328 Validation accuracy: 0.539419\n",
      "22 Train accuracy: 0.612299 Validation accuracy: 0.595436\n",
      "23 Train accuracy: 0.617647 Validation accuracy: 0.605809\n",
      "24 Train accuracy: 0.649733 Validation accuracy: 0.645228\n",
      "25 Train accuracy: 0.613191 Validation accuracy: 0.60166\n",
      "26 Train accuracy: 0.674688 Validation accuracy: 0.659751\n",
      "27 Train accuracy: 0.525847 Validation accuracy: 0.537344\n",
      "28 Train accuracy: 0.52139 Validation accuracy: 0.543568\n",
      "29 Train accuracy: 0.635472 Validation accuracy: 0.626556\n",
      "30 Train accuracy: 0.560606 Validation accuracy: 0.556017\n",
      "31 Train accuracy: 0.606952 Validation accuracy: 0.620332\n",
      "32 Train accuracy: 0.532977 Validation accuracy: 0.524896\n",
      "33 Train accuracy: 0.513369 Validation accuracy: 0.514523\n",
      "34 Train accuracy: 0.639037 Validation accuracy: 0.612033\n",
      "35 Train accuracy: 0.5918 Validation accuracy: 0.576764\n",
      "36 Train accuracy: 0.655971 Validation accuracy: 0.628631\n",
      "37 Train accuracy: 0.595365 Validation accuracy: 0.582988\n",
      "38 Train accuracy: 0.623886 Validation accuracy: 0.639004\n",
      "39 Train accuracy: 0.532977 Validation accuracy: 0.524896\n",
      "40 Train accuracy: 0.529412 Validation accuracy: 0.537344\n",
      "41 Train accuracy: 0.639929 Validation accuracy: 0.624481\n",
      "42 Train accuracy: 0.610517 Validation accuracy: 0.603734\n",
      "43 Train accuracy: 0.655971 Validation accuracy: 0.651452\n",
      "44 Train accuracy: 0.534759 Validation accuracy: 0.526971\n",
      "45 Train accuracy: 0.513369 Validation accuracy: 0.526971\n",
      "46 Train accuracy: 0.668449 Validation accuracy: 0.659751\n",
      "47 Train accuracy: 0.545455 Validation accuracy: 0.529046\n",
      "48 Train accuracy: 0.607843 Validation accuracy: 0.614108\n",
      "49 Train accuracy: 0.533868 Validation accuracy: 0.529046\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('training model\\n')\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op, feed_dict={X: X_train, y: y_train})   \n",
    "        acc_train = accuracy.eval(feed_dict={X: X_train, y: y_train})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_valid,\n",
    "                                            y: y_valid})\n",
    "    \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Validation accuracy:\", acc_test)\n",
    "    save_path = saver.save(sess, \"./cam_iceberg_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the test data\n",
    "\n",
    "As we did with the training and validation data, before making predictions I convert the type of the test data to float32.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8424, 75, 75, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#convert the test images to float32\n",
    "test_images =test_images.astype(np.float32) \n",
    "test_images.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "\n",
    "The last line y_pred = Z[:,1] selects the second column of the predictions because we want 'probability of iceberg' not 'probability of not iceberg' which would be column 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('making predictions\\n')\n",
    "#make external predictions on the test_dat\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./cam_iceberg_model_final.ckpt\") # or better, use save_path\n",
    "    Z = logits.eval(feed_dict={X: test_images}) #outputs switched to logits\n",
    "    y_pred = Z[:,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write output to file\n",
    "\n",
    "Lastly we take the predictions and construct a dataframe which we output to a .csv and can then submit for evalutation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "output = pd.DataFrame(test_df['id'])\n",
    "output['is_iceberg'] = y_pred\n",
    "\n",
    "output.to_csv('cam_tf_cnn.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
