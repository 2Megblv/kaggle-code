test_rmse = sqrt(test_mse)
test_rmse
bst_slow = xgb.train(data= gb_train, max.depth=6, eta=0.01, nthread = 2, nround=10000, watchlist=watchlist, objective = "reg:linear", early_stopping_rounds = 50)#
skrrrahh('ross')
skrrrahh(0) #random
bst_slow = xgb.train(data= gb_train, max.depth=6, eta=0.01, nthread = 2, nround=10000, watchlist= gb_valid, objective = "reg:linear", early_stopping_rounds = 50)
watchlist = list(train= gb_train, valid= gb_valid)
bst_slow = xgb.train(data= gb_train, max.depth=6, eta=0.01, nthread = 2, nround=10000, watchlist=watchlist, objective = "reg:linear", early_stopping_rounds = 50)
xgb_grid_1 = expand.grid(#
nrounds = 1000,#
eta = c(0.01, 0.001, 0.0001),#
max_depth = c(2, 4, 6, 8, 10),#
gamma = 1#
)
xgb_grid_1
xgb_train_1 = train(dtrain,#
					trControl = xgb_trcontrol_1,#
					tuneGrid = xgb_grid_1,#
					method = "xgbTree")
xgb_grid_1 = expand.grid(nrounds = c(1000,10000) ,#
							eta = c(0.01, 0.001, 0.0001),#
							max_depth = c(2, 4, 6, 8, 10),#
							gamma = 1,#
							early_stopping_rounds = c(25,50,100))
xgb_grid_1
xgb_trcontrol_1 = trainControl(method = "cv",#
								number = 5,#
								verboseIter = TRUE,#
								returnData = FALSE,#
								returnResamp = "all",   #
								classProbs = TRUE,  #
								summaryFunction = twoClassSummary,#
								allowParallel = TRUE#
								)
library(caret)
xgb_trcontrol_1 = trainControl(method = "cv",#
								number = 5,#
								verboseIter = TRUE,#
								returnData = FALSE,#
								returnResamp = "all",   #
								classProbs = TRUE,  #
								summaryFunction = twoClassSummary,#
								allowParallel = TRUE#
								)
xgb_trcontrol_1
xgb_grid_1
xgb_train_1 = train(dtrain,#
					trControl = xgb_trcontrol_1,#
					tuneGrid = xgb_grid_1,#
					method = "xgbTree")
dtrain = xgb.DMatrix(data =  as.matrix(train_x), label = train_y )
xgb_train_1 = train(dtrain,#
					trControl = xgb_trcontrol_1,#
					tuneGrid = xgb_grid_1,#
					method = "xgbTree")
?train
xgb_train_1 = train(x = as.matrix(train_x),#
					y = train_y#
					trControl = xgb_trcontrol_1,#
					tuneGrid = xgb_grid_1,#
					method = "xgbTree")
xgb_train_1 = train(x = as.matrix(train_x),#
					y = train_y,#
					trControl = xgb_trcontrol_1,#
					tuneGrid = xgb_grid_1,#
					method = "xgbTree")
xgb_train_1
?train #note how this is now the caret train function
xgb_train_1 = train(x = as.matrix(train_x),#
					y = train_y,#
					trControl = xgb_trcontrol_1,#
					tuneGrid = xgb_grid_1,#
					method = "xgbLinear")
skrrrahh(0) #random
skrrrahh('ross')
skrrrahh('traviscott')
skrrrahh('desiigner')
xgb_train_1 = train(x = as.matrix(train_x),#
					y = train_y,#
					trControl = xgb_trcontrol_1,#
					tuneGrid = xgb_grid_1,#
					method = "xgbLinear")
xgb_trcontrol_1 = trainControl(method = "cv",#
								number = 5,#
								verboseIter = TRUE,#
								returnData = FALSE,#
								returnResamp = "all",  #
								summaryFunction = twoClassSummary,#
								allowParallel = TRUE#
								)
modelLookup("xgbLinear")
xgb_grid_1 = expand.grid(nrounds = c(1000,10000) ,#
							eta = c(0.01, 0.001, 0.0001),#
							max_depth = c(2, 4, 6, 8, 10),#
							early_stopping_rounds = c(25,50,100))
xgb_grid_1
xgb_trcontrol_1 = trainControl(method = "cv",#
								number = 5,#
								verboseIter = TRUE,#
								returnData = FALSE,#
								returnResamp = "all",  #
								summaryFunction = twoClassSummary,#
								allowParallel = TRUE#
								)
xgb_grid_1 = expand.grid(nrounds = c(1000,10000) ,#
							eta = c(0.01, 0.001, 0.0001),#
							max_depth = c(2, 4, 6, 8, 10),#
							early_stopping_rounds = c(2
)
xgb_grid_1 = expand.grid(nrounds = c(1000,10000) ,#
							eta = c(0.01, 0.001, 0.0001),#
							max_depth = c(2, 4, 6, 8, 10),#
							early_stopping_rounds = c(25,50,100))
xgb_grid_1
xgb_trcontrol_1 = trainControl(method = "cv",#
								number = 5,#
								verboseIter = TRUE,#
								returnData = FALSE,#
								returnResamp = "all", #
								allowParallel = TRUE#
								)
xgb_train_1 = train(x = as.matrix(train_x),#
					y = train_y,#
					trControl = xgb_trcontrol_1,#
					tuneGrid = xgb_grid_1,#
					method = "xgbLinear")
xgb_grid_1 = expand.grid(nrounds = c(1000,10000) ,#
							eta = c(0.01, 0.001, 0.0001),#
							lambda = 1,#
							alpha = 0,#
							max_depth = c(2, 4, 6, 8, 10),#
							early_stopping_rounds = c(25,50,100))
xgb_trcontrol_1 = trainControl(method = "cv",#
								number = 5,#
								verboseIter = TRUE,#
								returnData = FALSE,#
								returnResamp = "all", #
								allowParallel = TRUE#
								)
xgb_train_1 = train(x = as.matrix(train_x),#
					y = train_y,#
					trControl = xgb_trcontrol_1,#
					tuneGrid = xgb_grid_1,#
					method = "xgbLinear")
xgb_grid_1 = expand.grid(nrounds = c(1000,10000) ,#
							eta = c(0.01, 0.001, 0.0001),#
							lambda = 1,#
							alpha = 0)
xgb_train_1 = train(x = as.matrix(train_x),#
					y = train_y,#
					trControl = xgb_trcontrol_1,#
					tuneGrid = xgb_grid_1,#
					method = "xgbLinear")
kill
kill#
iii
names(xgb_train_1)
xgb_train_1$bestTune
summary(xgb_train_1)
xgb_train_1$modelInfo
xgb_train_1$modeType
names(xgb_train_1)
xgb_train_1$method
xgb_cv_yhat = predict(xgb_train_1 , as.matrix(test_x))
xgb_cv_yhat
test_mse = mean(((xgb_cv_yhat - test_y)^2))
test_rmse = sqrt(test_mse)
test_rmse
xgb_train_1 = train(x = as.matrix(train_x),#
					y = train_y,#
					trControl = xgb_trcontrol_1,#
					tuneGrid = xgb_grid_1,#
					method = "xgbLinear",#
					early_stopping_rounds = 50,#
					max_depth = 5)
? train
xgb_trcontrol_1 = trainControl(method = "cv",#
								number = 5,#
								verboseIter = TRUE,#
								returnData = FALSE,#
								returnResamp = "all", #
								allowParallel = TRUE,#
								maximize = "RMSE"#
								early_stopping_rounds = 50,#
								max_depth = 5#
								)
xgb_trcontrol_1 = trainControl(method = "cv",#
								number = 5,#
								verboseIter = TRUE,#
								returnData = FALSE,#
								returnResamp = "all", #
								allowParallel = TRUE,#
								maximize = "RMSE",#
								early_stopping_rounds = 50,#
								max_depth = 5)
xgb_train_1 = train(x = as.matrix(train_x),#
					y = train_y,#
					trControl = xgb_trcontrol_1,#
					tuneGrid = xgb_grid_1,#
					method = "xgbLinear",#
					maximize = "RMSE"#
					early_stopping_rounds = 50,#
					max_depth = 5)
xgb_trcontrol_1 = trainControl(method = "cv",#
								number = 5,#
								verboseIter = TRUE,#
								returnData = FALSE,#
								returnResamp = "all", #
								allowParallel = TRUE)
xgb_grid_1 = expand.grid(nrounds = c(1000,10000) ,#
							eta = c(0.01, 0.001, 0.0001),#
							lambda = 1,#
							alpha = 0)
xgb_train_1 = train(x = as.matrix(train_x),#
					y = train_y,#
					trControl = xgb_trcontrol_1,#
					tuneGrid = xgb_grid_1,#
					method = "xgbLinear",#
					maximize = "RMSE"#
					early_stopping_rounds = 50,#
					max_depth = 5)
xgb_train_1 = train(x = as.matrix(train_x),#
					y = train_y,#
					trControl = xgb_trcontrol_1,#
					tuneGrid = xgb_grid_1,#
					method = "xgbLinear",#
					maximize = "RMSE",#
					early_stopping_rounds = 50,#
					max_depth = 5)
warnings()
?train
bst_slow
names(bst_slow)
?xgb.train
max.depths = c(3,5,7,9)#
etas = c(0.01, 0.001, 0.0001)#
#
params_list = rep(0, times=12)#
scores_list = rep(0, times=12)#
#
count = 1#
for( depth in max_depths ){#
	for( weight in etas){#
		bst_grid = xgb.train(data= gb_train, max.depth=6, eta=0.01, nthread = 2, nround=10000, watchlist=watchlist, objective = "reg:linear", early_stopping_rounds = 50, verbose=0)#
		params_list[count] = bst_grid$"params"#
		scores_list[count] = bst_grid$"best_score" #
	}#
}
max.depths = c(3,5,7,9)#
etas = c(0.01, 0.001, 0.0001)#
#
params_list = rep(0, times=12)#
scores_list = rep(0, times=12)#
#
count = 1#
for( depth in max.depths ){#
	for( num in etas){#
		bst_grid = xgb.train(data= gb_train, max.depth=depth, eta=num, nthread = 2, nround=10000, watchlist=watchlist, objective = "reg:linear", early_stopping_rounds = 50, verbose=0)#
		params_list[count] = bst_grid$"params"#
		scores_list[count] = bst_grid$"best_score" #
		count += 1#
	}#
}
max.depths = c(3,5,7,9)#
etas = c(0.01, 0.001, 0.0001)#
#
params_list = rep(0, times=12)#
scores_list = rep(0, times=12)#
#
count = 1#
for( depth in max.depths ){#
	for( num in etas){#
		bst_grid = xgb.train(data= gb_train, max.depth=depth, eta=num, nthread = 2, nround=10000, watchlist=watchlist, objective = "reg:linear", early_stopping_rounds = 50, verbose=0)#
		params_list[count] = bst_grid$params#
		scores_list[count] = bst_grid$best_score#
		count += 1#
	}#
}
depth
params_list[count]
gb_train
depth
num
watchlist = list(train = gb_train, valid = gb_valid)
max.depths = c(3,5,7,9)#
etas = c(0.01, 0.001, 0.0001)#
#
params_list = rep(0, times=12)#
scores_list = rep(0, times=12)
count = 1
for( depth in max.depths ){#
	for( num in etas){#
		bst_grid = xgb.train(data= gb_train, max.depth=depth, eta=num, nthread = 2, nround=10000, watchlist=watchlist, objective = "reg:linear", early_stopping_rounds = 50, verbose=0)#
		params_list[count] = bst_grid$params#
		scores_list[count] = bst_grid$best_score#
		count += 1#
	}#
}
max.depths = c(3,5,7,9)#
etas = c(0.01, 0.001, 0.0001)#
#
params_list = rep(0, times=12)#
scores_list = rep(0, times=12)#
#
count = 1#
for( depth in max.depths ){#
	for( num in etas){#
		bst_grid = xgb.train(data= gb_train, max.depth=depth, eta=num, nthread = 2, nround=10000, watchlist=watchlist, objective = "reg:linear", early_stopping_rounds = 50, verbose=0)#
		params_list[count] = bst_grid$params#
		scores_list[count] = bst_grid$best_score#
		count = count + 1#
	}#
}
scores_list
params_list
scores_list
params_list
bst_grid$params
params_list[1,]
params_list[1,:]
params_list[1][:]
params_list[1]
params_list[,1]
params_list[[1]]
max.depths = c(3,5,7,9)#
etas = c(0.01, 0.001, 0.0001)#
#
best_params = 0#
best_score = 0#
#
count = 1#
for( depth in max.depths ){#
	for( num in etas){#
		bst_grid = xgb.train(data= gb_train, max.depth=depth, eta=num, nthread = 2, nround=10000, watchlist=watchlist, objective = "reg:linear", early_stopping_rounds = 50, verbose=0)#
		if(count = 1){#
			best_params = bst_grid$params#
			best_score = bst_grid$best_score#
			count = count + 1#
			}#
		else if(bst_grid$best_score < best_score){#
			print(bst_grid$best_score)#
			best_params = bst_grid$params#
			best_score = bst_grid$best_score#
		}	#
	}#
}
max.depths = c(3,5,7,9)#
etas = c(0.01, 0.001, 0.0001)#
#
best_params = 0#
best_score = 0#
#
count = 1#
for( depth in max.depths ){#
	for( num in etas){#
		bst_grid = xgb.train(data= gb_train, max.depth=depth, eta=num, nthread = 2, nround=10000, watchlist=watchlist, objective = "reg:linear", early_stopping_rounds = 50, verbose=0)#
		if(count = 1){#
			best_params = bst_grid$params#
			best_score = bst_grid$best_score#
			count = count + 1#
			}#
		else if(bst_grid$best_score < best_score){#
			best_params = bst_grid$params#
			best_score = bst_grid$best_score#
		}	#
	}#
}
max.depths = c(3,5,7,9)#
etas = c(0.01, 0.001, 0.0001)#
#
best_params = 0#
best_score = 0#
#
count = 1#
for( depth in max.depths ){#
	for( num in etas){#
#
		bst_grid = xgb.train(data= gb_train, max.depth=depth, eta=num, nthread = 2, nround=10000, watchlist=watchlist, objective = "reg:linear", early_stopping_rounds = 50, verbose=0)#
#
		if(count == 1){#
			best_params = bst_grid$params#
			best_score = bst_grid$best_score#
			count = count + 1#
			}#
		else if(bst_grid$best_score < best_score){#
			best_params = bst_grid$params#
			best_score = bst_grid$best_score#
		}	#
	}#
}
best_params
best_score
bst_tuned = xgb.train( data = gb_train, #
						max.depth = 9, #
						eta = 0.01, #
						nthread = 2, #
						nround = 10000, #
						watchlist = watchlist, #
						objective = "reg:linear", #
						early_stopping_rounds = 50)
best_score
y_hat_xgb_grid = predict(bst_tuned, dtest)
test_mse = mean(((y_hat_xgb_grid - test_y)^2))
test_rmse = sqrt(test_mse)
test_rmse
bst_tuned = xgb.train( data = gb_train, #
						max.depth = 6, #
						eta = 0.01, #
						nthread = 2, #
						nround = 10000, #
						watchlist = watchlist, #
						objective = "reg:linear", #
						early_stopping_rounds = 50)
y_hat_xgb_grid = predict(bst_tuned, dtest)
test_mse = mean(((y_hat_xgb_grid - test_y)^2))
test_rmse = sqrt(test_mse)
test_rmse
bst_slow = xgb.train(data= gb_train, #
						max.depth = 10, #
						eta = 0.01, #
						nthread = 2, #
						nround = 10000, #
						watchlist = watchlist, #
						objective = "reg:linear", #
						early_stopping_rounds = 50)
y_hat = predict(bst_slow, dtest)
test_mse = mean(((y_hat - test_y)^2))
test_rmse = sqrt(test_mse)
test_rmse
45225+3400
test_rmse
47507/48620
y_hat_xgb_grid
test_mse = mean(((y_hat_xgb_grid - test_y)^2))
test_rmse = sqrt(test_mse)
test_rmse
bst_tuned = xgb.train( data = gb_train, #
						max.depth = 9, #
						eta = 0.01, #
						nthread = 2, #
						nround = 10000, #
						watchlist = watchlist, #
						objective = "reg:linear", #
						early_stopping_rounds = 50)#
#
y_hat_xgb_grid = predict(bst_tuned, dtest)#
#
test_mse = mean(((y_hat_xgb_grid - test_y)^2))#
test_rmse = sqrt(test_mse)#
test_rmse
46675/48620
y_hat
y_pred_rf
xgb_cv_yhat
y_hat_xgb_grid
rf_model = randomForest(train_x, y = train_y , ntree = 500, importance = TRUE)
names(rf_model) #these are all the different things you can call from the model.
importance_dat = rf_model$importance
importance_dat
train_y = train[,'median_house_value']
train_x = train[, names(train) !='median_house_value']
rf_model = randomForest(train_x, y = train_y , ntree = 500, importance = TRUE)
importance_dat = rf_model$importance
importance_dat
sorted_predictors = sort(importance_dat[,1], decreasing=TRUE)
sorted_predictors
oob_prediction = predict(rf_model) #leaving out a data source forces OOB predictions
train_mse = mean(as.numeric((oob_prediction - train_y)^2))
oob_rmse = sqrt(train_mse)
oob_rmse
y_pred_rf = predict(rf_model , test_x)
test_mse = mean(((y_pred_rf - test_y)^2))
test_rmse = sqrt(test_mse)
test_rmse
xgb_cv_yhat
test_mse = mean(((xgb_cv_yhat - test_y)^2))
test_rmse = sqrt(test_mse)
test_rmse
blend_pred
blend_pred = (y_hat * .25) + (y_pred_rf * .25) + (xgb_cv_yhat * .25) + (y_hat_xgb_grid * .25)
blend_pred
y_hat_xgb_grid
blend_pred
length(blend_pred)
length(y_hat_xgb_grid)
blend_test_rmse = sqrt(blend_test_mse)
blend_test_mse = mean(((blend_pred - test_y)^2))
blend_test_rmse = sqrt(blend_test_mse)
blend_test_rmse
bst_slow = xgb.train(data= gb_train, #
						max.depth = 10, #
						eta = 0.01, #
						nthread = 2, #
						nround = 10000, #
						watchlist = watchlist, #
						objective = "reg:linear", #
						early_stopping_rounds = 50)#
#
# error, need the matrix format#
y_hat = predict(bst_slow, test_x)#
#
# recall we ran the following to get the test data in the right format:#
# dtest = xgb.DMatrix(data =  as.matrix(test_x), label = test_y)#
# here I have it with the label taken off, just to remind us its external data xgb would ignore the label though during predictions#
dtest = xgb.DMatrix(data =  as.matrix(test_x))#
#
#test the model on truly external data#
#
y_hat_valid = predict(bst_slow, dtest)
test_mse = mean(((y_hat_valid - test_y)^2))
test_rmse = sqrt(test_mse)
test_rmse
y_hat_valid
blend_pred = (y_hat * .25) + (y_pred_rf * .25) + (xgb_cv_yhat * .25) + (y_hat_xgb_grid * .25)#
length(blend_pred)#
#
blend_test_mse = mean(((blend_pred - test_y)^2))#
blend_test_rmse = sqrt(blend_test_mse)#
blend_test_rmse # 45907 by averaging just 4 predictors we have dropped the rmse by
45907/46675
length(blend_pred) == length(y_hat_xgb_grid)
xgb_grid_1 = expand.grid(nrounds = c(1000,2000,3000,4000) ,#
							eta = c(0.01, 0.001, 0.0001),#
							lambda = 1,#
							alpha = 0)
xgb_trcontrol_1 = trainControl(method = "cv",#
								number = 5,#
								verboseIter = TRUE,#
								returnData = FALSE,#
								returnResamp = "all", #
								allowParallel = TRUE)
xgb_train_1 = train(x = as.matrix(train_x),#
					y = train_y,#
					trControl = xgb_trcontrol_1,#
					tuneGrid = xgb_grid_1,#
					method = "xgbLinear",#
					maximize = "RMSE",#
					early_stopping_rounds = 50,#
					max.depth = 5)
xgb_train_1 = train(x = as.matrix(train_x),#
					y = train_y,#
					trControl = xgb_trcontrol_1,#
					tuneGrid = xgb_grid_1,#
					method = "xgbLinear",#
					early_stopping_rounds = 50,#
					max.depth = 5)
xgb_train_1 = train(x = as.matrix(train_x),#
					y = train_y,#
					trControl = xgb_trcontrol_1,#
					tuneGrid = xgb_grid_1,#
					method = "xgbLinear",#
					max.depth = 5)
xgb_train_1$bestTune
xgb_train_1$method
xgb_cv_yhat = predict(xgb_train_1 , as.matrix(test_x))
test_mse = mean(((xgb_cv_yhat - test_y)^2))
test_rmse = sqrt(test_mse)
test_rmse # 48450... not really an improvement!
blend_pred = (y_hat * .25) + (y_pred_rf * .25) + (xgb_cv_yhat * .25) + (y_hat_xgb_grid * .25)
length(blend_pred)
length(blend_pred) == length(y_hat_xgb_grid)
blend_test_mse = mean(((blend_pred - test_y)^2))
blend_test_rmse = sqrt(blend_test_mse)
blend_test_rmse # 45907 by averaging just 4 predictors we have dropped the rmse ~1.5% lower then the best scoring of the 4 models. This does come at a cost though, we now can't make accurate inferrences about the best predictors!
45205/46641
library(viridis)
?viridis
setwd('/Users/Cam/Desktop/Code/bin/kaggle_code/california_housing')
library(tidyverse)
housing.tidy = read_csv('housing.csv')
housing.tidy = housing.tidy %>% #
  mutate(total_bedrooms = ifelse(is.na(total_bedrooms), #
                                 median(total_bedrooms, na.rm = T),#
                                 total_bedrooms),#
         mean_bedrooms = total_bedrooms/households,#
         mean_rooms = total_rooms/households) %>%#
  select(-c(total_rooms, total_bedrooms))
categories = unique(housing.tidy$ocean_proximity) # all categories#
cat_housing.tidy = categories %>% # compare the full vector against each category consecutively#
  lapply(function(x) as.numeric(housing.tidy$ocean_proximity == x)) %>% # convert to numeric#
  do.call("cbind", .) %>% as.tibble() # clean up#
colnames(cat_housing.tidy) = categories # make nice column names
cat_housing.tidy
skrrrahh(0) #random
library('BRRR')
skrrrahh(0) #random
train
setwd('/Users/Cam/Desktop/Code/bin/kaggle_code/california_housing')#
library(tidyverse)#
library(reshape2)#
library(boot)#
library(randomForest)#
#
housing = read.csv('./housing.csv')#
#
housing$total_bedrooms[is.na(housing$total_bedrooms)] = median(housing$total_bedrooms , na.rm = TRUE)#
#
housing$mean_bedrooms = housing$total_bedrooms/housing$households#
housing$mean_rooms = housing$total_rooms/housing$households#
#
drops = c('total_bedrooms', 'total_rooms')#
#
housing = housing[ , !(names(housing) %in% drops)]#
#
categories = unique(housing$ocean_proximity)#
#split the categories off#
cat_housing = data.frame(ocean_proximity = housing$ocean_proximity)#
for(cat in categories){#
    cat_housing[,cat] = rep(0, times= nrow(cat_housing))#
}#
head(cat_housing) #see the new columns on the right#
#
for(i in 1:length(cat_housing$ocean_proximity)){#
    cat = as.character(cat_housing$ocean_proximity[i])#
    cat_housing[,cat][i] = 1#
}#
cat_columns = names(cat_housing)#
keep_columns = cat_columns[cat_columns != 'ocean_proximity']#
cat_housing = select(cat_housing,one_of(keep_columns))#
drops = c('ocean_proximity','median_house_value')#
housing_num =  housing[ , !(names(housing) %in% drops)]#
scaled_housing_num = scale(housing_num)#
#
cleaned_housing = cbind(cat_housing, scaled_housing_num, median_house_value=housing$median_house_value)#
set.seed(19) # Set a random seed so that same sample can be reproduced in future runs#
#
sample = sample.int(n = nrow(cleaned_housing), size = floor(.8*nrow(cleaned_housing)), replace = F)#
train = cleaned_housing[sample, ] #just the samples#
test  = cleaned_housing[-sample, ] #everything but the samples#
train_y = train[,'median_house_value']#
train_x = train[, names(train) !='median_house_value']#
#
test_y = test[,'median_house_value']#
test_x = test[, names(test) !='median_house_value']
train_t = train[sample, ] #just the samples
valid  = train[-sample, ] #everything but the samples
train_t = train[sample, ] #just the samples
sample = sample.int(n = nrow(train), size = floor(.8*nrow(train)), replace = F)
train_t = train[sample, ] #just the samples
valid  = train[-sample, ] #everything but the samples
train_t
valid  = train[-sample, ] #everything but the samples
train_y = train_t[,'median_house_value']
train_x = train_t[, names(train_t) !='median_house_value']
valid_y = valid[,'median_house_value']
valid_x = valid[, names(train_t) !='median_house_value']
gb_train = xgb.DMatrix(data = as.matrix(train_x), label = train_y )
library(xgboost)
gb_train = xgb.DMatrix(data = as.matrix(train_x), label = train_y )
gb_valid = xgb.DMatrix(data = as.matrix(valid_x), label = valid_y)
watchlist = list(train = gb_train, valid = gb_valid)
valid_y
library('BRRR')
skrrrahh(0) #random
skrrrahh('ross')
skrrrahh('future')
skrrrahh('traviscott')
skrrrahh('desiigner')
skrrrahh('bigshaq')
skrrrahh('twochainz1')
skrrrahh('bigsean3')
skrrrahh('desiigner')
skrrrahh(0) #random
setwd('/Users/Cam/Desktop/Code/bin/kaggle_code/california_housing')
library(tidyverse)
library(reshape2)
library(boot)
library(randomForest)
skrrrahh('future')
library('BRRR')
skrrrahh('desiigner')
library('BRRR')
skrrrahh('ross')
skrrrahh('traviscott')
skrrrahh('desiigner')
