{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in R: part 2\n",
    "\n",
    "## This week focuses on improvement. We will our data cleaning code more efficient and then use some tuning methods to improve the predictive ability of models we build.\n",
    "\n",
    "\n",
    "## 1a. Cleaning and formatting the data (from last week)\n",
    "\n",
    "Below is the code from the loading, cleaning and train/test split sections we went over last week. This is all we requre to get the data into the format needed to being training some machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading tidyverse: ggplot2\n",
      "Loading tidyverse: tibble\n",
      "Loading tidyverse: tidyr\n",
      "Loading tidyverse: readr\n",
      "Loading tidyverse: purrr\n",
      "Loading tidyverse: dplyr\n",
      "Conflicts with tidy packages ---------------------------------------------------\n",
      "filter(): dplyr, stats\n",
      "lag():    dplyr, stats\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>NEAR BAY</th><th scope=col>&lt;1H OCEAN</th><th scope=col>INLAND</th><th scope=col>NEAR OCEAN</th><th scope=col>ISLAND</th><th scope=col>longitude</th><th scope=col>latitude</th><th scope=col>housing_median_age</th><th scope=col>population</th><th scope=col>households</th><th scope=col>median_income</th><th scope=col>mean_bedrooms</th><th scope=col>mean_rooms</th><th scope=col>median_house_value</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>2418</th><td>0          </td><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td> 0.06473791</td><td> 0.4485767 </td><td>-0.05081113</td><td>-0.08342596</td><td>-0.50882695</td><td>-1.2394168 </td><td>-0.03648780</td><td>-0.4145713 </td><td> 56700     </td></tr>\n",
       "\t<tr><th scope=row>9990</th><td>0          </td><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td>-0.74882545</td><td> 1.6471053 </td><td>-1.08374113</td><td> 1.39212008</td><td> 2.14071836</td><td>-0.7358959 </td><td>-0.19291092</td><td>-0.1004065 </td><td>143400     </td></tr>\n",
       "\t<tr><th scope=row>13440</th><td>0          </td><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td> 1.07295753</td><td>-0.7218613 </td><td>-0.05081113</td><td> 0.28656434</td><td> 0.06136148</td><td> 0.1404495 </td><td>-0.18700644</td><td> 0.2732884 </td><td>128300     </td></tr>\n",
       "\t<tr><th scope=row>1412</th><td>1          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>-1.25293526</td><td> 1.0759315 </td><td> 0.50538194</td><td> 0.35897294</td><td> 0.42492199</td><td> 0.6344959 </td><td>-0.11581168</td><td> 0.2741324 </td><td>233200     </td></tr>\n",
       "\t<tr><th scope=row>7539</th><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td>0          </td><td> 0.67865382</td><td>-0.8061329 </td><td>-0.20972344</td><td> 1.03802435</td><td> 0.21829408</td><td>-1.0991931 </td><td>-0.03247975</td><td>-0.5151724 </td><td>110200     </td></tr>\n",
       "\t<tr><th scope=row>4621</th><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td>0          </td><td> 0.62874196</td><td>-0.7265431 </td><td> 1.61776810</td><td> 0.10024464</td><td> 0.24706505</td><td>-0.6573622 </td><td>-0.07763347</td><td>-0.4598522 </td><td>350900     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllll}\n",
       "  & NEAR BAY & <1H OCEAN & INLAND & NEAR OCEAN & ISLAND & longitude & latitude & housing\\_median\\_age & population & households & median\\_income & mean\\_bedrooms & mean\\_rooms & median\\_house\\_value\\\\\n",
       "\\hline\n",
       "\t2418 & 0           & 0           & 1           & 0           & 0           &  0.06473791 &  0.4485767  & -0.05081113 & -0.08342596 & -0.50882695 & -1.2394168  & -0.03648780 & -0.4145713  &  56700     \\\\\n",
       "\t9990 & 0           & 0           & 1           & 0           & 0           & -0.74882545 &  1.6471053  & -1.08374113 &  1.39212008 &  2.14071836 & -0.7358959  & -0.19291092 & -0.1004065  & 143400     \\\\\n",
       "\t13440 & 0           & 0           & 1           & 0           & 0           &  1.07295753 & -0.7218613  & -0.05081113 &  0.28656434 &  0.06136148 &  0.1404495  & -0.18700644 &  0.2732884  & 128300     \\\\\n",
       "\t1412 & 1           & 0           & 0           & 0           & 0           & -1.25293526 &  1.0759315  &  0.50538194 &  0.35897294 &  0.42492199 &  0.6344959  & -0.11581168 &  0.2741324  & 233200     \\\\\n",
       "\t7539 & 0           & 1           & 0           & 0           & 0           &  0.67865382 & -0.8061329  & -0.20972344 &  1.03802435 &  0.21829408 & -1.0991931  & -0.03247975 & -0.5151724  & 110200     \\\\\n",
       "\t4621 & 0           & 1           & 0           & 0           & 0           &  0.62874196 & -0.7265431  &  1.61776810 &  0.10024464 &  0.24706505 & -0.6573622  & -0.07763347 & -0.4598522  & 350900     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | NEAR BAY | <1H OCEAN | INLAND | NEAR OCEAN | ISLAND | longitude | latitude | housing_median_age | population | households | median_income | mean_bedrooms | mean_rooms | median_house_value | \n",
       "|---|---|---|---|---|---|\n",
       "| 2418 | 0           | 0           | 1           | 0           | 0           |  0.06473791 |  0.4485767  | -0.05081113 | -0.08342596 | -0.50882695 | -1.2394168  | -0.03648780 | -0.4145713  |  56700      | \n",
       "| 9990 | 0           | 0           | 1           | 0           | 0           | -0.74882545 |  1.6471053  | -1.08374113 |  1.39212008 |  2.14071836 | -0.7358959  | -0.19291092 | -0.1004065  | 143400      | \n",
       "| 13440 | 0           | 0           | 1           | 0           | 0           |  1.07295753 | -0.7218613  | -0.05081113 |  0.28656434 |  0.06136148 |  0.1404495  | -0.18700644 |  0.2732884  | 128300      | \n",
       "| 1412 | 1           | 0           | 0           | 0           | 0           | -1.25293526 |  1.0759315  |  0.50538194 |  0.35897294 |  0.42492199 |  0.6344959  | -0.11581168 |  0.2741324  | 233200      | \n",
       "| 7539 | 0           | 1           | 0           | 0           | 0           |  0.67865382 | -0.8061329  | -0.20972344 |  1.03802435 |  0.21829408 | -1.0991931  | -0.03247975 | -0.5151724  | 110200      | \n",
       "| 4621 | 0           | 1           | 0           | 0           | 0           |  0.62874196 | -0.7265431  |  1.61776810 |  0.10024464 |  0.24706505 | -0.6573622  | -0.07763347 | -0.4598522  | 350900      | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "      NEAR BAY <1H OCEAN INLAND NEAR OCEAN ISLAND longitude   latitude  \n",
       "2418  0        0         1      0          0       0.06473791  0.4485767\n",
       "9990  0        0         1      0          0      -0.74882545  1.6471053\n",
       "13440 0        0         1      0          0       1.07295753 -0.7218613\n",
       "1412  1        0         0      0          0      -1.25293526  1.0759315\n",
       "7539  0        1         0      0          0       0.67865382 -0.8061329\n",
       "4621  0        1         0      0          0       0.62874196 -0.7265431\n",
       "      housing_median_age population  households  median_income mean_bedrooms\n",
       "2418  -0.05081113        -0.08342596 -0.50882695 -1.2394168    -0.03648780  \n",
       "9990  -1.08374113         1.39212008  2.14071836 -0.7358959    -0.19291092  \n",
       "13440 -0.05081113         0.28656434  0.06136148  0.1404495    -0.18700644  \n",
       "1412   0.50538194         0.35897294  0.42492199  0.6344959    -0.11581168  \n",
       "7539  -0.20972344         1.03802435  0.21829408 -1.0991931    -0.03247975  \n",
       "4621   1.61776810         0.10024464  0.24706505 -0.6573622    -0.07763347  \n",
       "      mean_rooms median_house_value\n",
       "2418  -0.4145713  56700            \n",
       "9990  -0.1004065 143400            \n",
       "13440  0.2732884 128300            \n",
       "1412   0.2741324 233200            \n",
       "7539  -0.5151724 110200            \n",
       "4621  -0.4598522 350900            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "\n",
    "housing = read.csv('./housing.csv')\n",
    "\n",
    "housing$total_bedrooms[is.na(housing$total_bedrooms)] = median(housing$total_bedrooms , na.rm = TRUE)\n",
    "\n",
    "housing$mean_bedrooms = housing$total_bedrooms/housing$households\n",
    "housing$mean_rooms = housing$total_rooms/housing$households\n",
    "\n",
    "drops = c('total_bedrooms', 'total_rooms')\n",
    "\n",
    "housing = housing[ , !(names(housing) %in% drops)]\n",
    "\n",
    "categories = unique(housing$ocean_proximity)\n",
    "#split the categories off\n",
    "cat_housing = data.frame(ocean_proximity = housing$ocean_proximity)\n",
    "\n",
    "for(cat in categories){\n",
    "    cat_housing[,cat] = rep(0, times= nrow(cat_housing))\n",
    "}\n",
    "\n",
    "for(i in 1:length(cat_housing$ocean_proximity)){\n",
    "    cat = as.character(cat_housing$ocean_proximity[i])\n",
    "    cat_housing[,cat][i] = 1\n",
    "}\n",
    "\n",
    "cat_columns = names(cat_housing)\n",
    "keep_columns = cat_columns[cat_columns != 'ocean_proximity']\n",
    "cat_housing = select(cat_housing,one_of(keep_columns))\n",
    "drops = c('ocean_proximity','median_house_value')\n",
    "housing_num =  housing[ , !(names(housing) %in% drops)]\n",
    "\n",
    "\n",
    "scaled_housing_num = scale(housing_num)\n",
    "\n",
    "cleaned_housing = cbind(cat_housing, scaled_housing_num, median_house_value=housing$median_house_value)\n",
    "\n",
    "\n",
    "set.seed(19) # Set a random seed so that same sample can be reproduced in future runs\n",
    "\n",
    "sample = sample.int(n = nrow(cleaned_housing), size = floor(.8*nrow(cleaned_housing)), replace = F)\n",
    "train = cleaned_housing[sample, ] #just the samples\n",
    "test  = cleaned_housing[-sample, ] #everything but the samples\n",
    "\n",
    "\n",
    "train_y = train[,'median_house_value']\n",
    "train_x = train[, names(train) !='median_house_value']\n",
    "\n",
    "test_y = test[,'median_house_value']\n",
    "test_x = test[, names(test) !='median_house_value']\n",
    "\n",
    "head(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Cleaning - The tidyverse way! \n",
    "\n",
    "The code below does the same thing as the code above, but employs the tidyverse. I've pulled out the bare bones parts of Karl's 'Housing_R_tidy.r' script needed to get the data to where we want it (i.e. removed all the graphs head commands etc. Go to his original script to see the notes and visual blandishments).\n",
    "\n",
    "I like this code more then my original version because:\n",
    "1. It is easy to follow the workflow. magrittr makes it easy to see when one cleaning task ends and the next begins.\n",
    "2. It is more concise.\n",
    "3. The use of comments(#) after the pipes(%>%) looks professional and also makes the code more readable. Being able to share your code with others and have them understand it is very important!\n",
    "4. It runs faster.\n",
    "\n",
    "Note: in the tibble docs the function is listed as: as_tibble() not as.tibble() as first written. Oddly as.tibble() worked in my normal R deployment, but threw an error in the jupyter notebook :\\ This confused me and I don't know what to make of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsed with column specification:\n",
      "cols(\n",
      "  longitude = col_double(),\n",
      "  latitude = col_double(),\n",
      "  housing_median_age = col_double(),\n",
      "  total_rooms = col_double(),\n",
      "  total_bedrooms = col_double(),\n",
      "  population = col_double(),\n",
      "  households = col_double(),\n",
      "  median_income = col_double(),\n",
      "  median_house_value = col_double(),\n",
      "  ocean_proximity = col_character()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>longitude</th><th scope=col>latitude</th><th scope=col>housing_median_age</th><th scope=col>population</th><th scope=col>households</th><th scope=col>median_income</th><th scope=col>mean_bedrooms</th><th scope=col>mean_rooms</th><th scope=col>NEAR BAY</th><th scope=col>&lt;1H OCEAN</th><th scope=col>INLAND</th><th scope=col>NEAR OCEAN</th><th scope=col>ISLAND</th><th scope=col>median_house_value</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 0.06473791</td><td> 0.4485767 </td><td>-0.05081113</td><td>-0.08342596</td><td>-0.50882695</td><td>-1.2394168 </td><td>-0.03648780</td><td>-0.4145713 </td><td>0          </td><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td> 56700     </td></tr>\n",
       "\t<tr><td>-0.74882545</td><td> 1.6471053 </td><td>-1.08374113</td><td> 1.39212008</td><td> 2.14071836</td><td>-0.7358959 </td><td>-0.19291092</td><td>-0.1004065 </td><td>0          </td><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td>143400     </td></tr>\n",
       "\t<tr><td> 1.07295753</td><td>-0.7218613 </td><td>-0.05081113</td><td> 0.28656434</td><td> 0.06136148</td><td> 0.1404495 </td><td>-0.18700644</td><td> 0.2732884 </td><td>0          </td><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td>128300     </td></tr>\n",
       "\t<tr><td>-1.25293526</td><td> 1.0759315 </td><td> 0.50538194</td><td> 0.35897294</td><td> 0.42492199</td><td> 0.6344959 </td><td>-0.11581168</td><td> 0.2741324 </td><td>1          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>233200     </td></tr>\n",
       "\t<tr><td> 0.67865382</td><td>-0.8061329 </td><td>-0.20972344</td><td> 1.03802435</td><td> 0.21829408</td><td>-1.0991931 </td><td>-0.03247975</td><td>-0.5151724 </td><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td>0          </td><td>110200     </td></tr>\n",
       "\t<tr><td> 0.62874196</td><td>-0.7265431 </td><td> 1.61776810</td><td> 0.10024464</td><td> 0.24706505</td><td>-0.6573622 </td><td>-0.07763347</td><td>-0.4598522 </td><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td>0          </td><td>350900     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllll}\n",
       " longitude & latitude & housing\\_median\\_age & population & households & median\\_income & mean\\_bedrooms & mean\\_rooms & NEAR BAY & <1H OCEAN & INLAND & NEAR OCEAN & ISLAND & median\\_house\\_value\\\\\n",
       "\\hline\n",
       "\t  0.06473791 &  0.4485767  & -0.05081113 & -0.08342596 & -0.50882695 & -1.2394168  & -0.03648780 & -0.4145713  & 0           & 0           & 1           & 0           & 0           &  56700     \\\\\n",
       "\t -0.74882545 &  1.6471053  & -1.08374113 &  1.39212008 &  2.14071836 & -0.7358959  & -0.19291092 & -0.1004065  & 0           & 0           & 1           & 0           & 0           & 143400     \\\\\n",
       "\t  1.07295753 & -0.7218613  & -0.05081113 &  0.28656434 &  0.06136148 &  0.1404495  & -0.18700644 &  0.2732884  & 0           & 0           & 1           & 0           & 0           & 128300     \\\\\n",
       "\t -1.25293526 &  1.0759315  &  0.50538194 &  0.35897294 &  0.42492199 &  0.6344959  & -0.11581168 &  0.2741324  & 1           & 0           & 0           & 0           & 0           & 233200     \\\\\n",
       "\t  0.67865382 & -0.8061329  & -0.20972344 &  1.03802435 &  0.21829408 & -1.0991931  & -0.03247975 & -0.5151724  & 0           & 1           & 0           & 0           & 0           & 110200     \\\\\n",
       "\t  0.62874196 & -0.7265431  &  1.61776810 &  0.10024464 &  0.24706505 & -0.6573622  & -0.07763347 & -0.4598522  & 0           & 1           & 0           & 0           & 0           & 350900     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "longitude | latitude | housing_median_age | population | households | median_income | mean_bedrooms | mean_rooms | NEAR BAY | <1H OCEAN | INLAND | NEAR OCEAN | ISLAND | median_house_value | \n",
       "|---|---|---|---|---|---|\n",
       "|  0.06473791 |  0.4485767  | -0.05081113 | -0.08342596 | -0.50882695 | -1.2394168  | -0.03648780 | -0.4145713  | 0           | 0           | 1           | 0           | 0           |  56700      | \n",
       "| -0.74882545 |  1.6471053  | -1.08374113 |  1.39212008 |  2.14071836 | -0.7358959  | -0.19291092 | -0.1004065  | 0           | 0           | 1           | 0           | 0           | 143400      | \n",
       "|  1.07295753 | -0.7218613  | -0.05081113 |  0.28656434 |  0.06136148 |  0.1404495  | -0.18700644 |  0.2732884  | 0           | 0           | 1           | 0           | 0           | 128300      | \n",
       "| -1.25293526 |  1.0759315  |  0.50538194 |  0.35897294 |  0.42492199 |  0.6344959  | -0.11581168 |  0.2741324  | 1           | 0           | 0           | 0           | 0           | 233200      | \n",
       "|  0.67865382 | -0.8061329  | -0.20972344 |  1.03802435 |  0.21829408 | -1.0991931  | -0.03247975 | -0.5151724  | 0           | 1           | 0           | 0           | 0           | 110200      | \n",
       "|  0.62874196 | -0.7265431  |  1.61776810 |  0.10024464 |  0.24706505 | -0.6573622  | -0.07763347 | -0.4598522  | 0           | 1           | 0           | 0           | 0           | 350900      | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  longitude   latitude   housing_median_age population  households \n",
       "1  0.06473791  0.4485767 -0.05081113        -0.08342596 -0.50882695\n",
       "2 -0.74882545  1.6471053 -1.08374113         1.39212008  2.14071836\n",
       "3  1.07295753 -0.7218613 -0.05081113         0.28656434  0.06136148\n",
       "4 -1.25293526  1.0759315  0.50538194         0.35897294  0.42492199\n",
       "5  0.67865382 -0.8061329 -0.20972344         1.03802435  0.21829408\n",
       "6  0.62874196 -0.7265431  1.61776810         0.10024464  0.24706505\n",
       "  median_income mean_bedrooms mean_rooms NEAR BAY <1H OCEAN INLAND NEAR OCEAN\n",
       "1 -1.2394168    -0.03648780   -0.4145713 0        0         1      0         \n",
       "2 -0.7358959    -0.19291092   -0.1004065 0        0         1      0         \n",
       "3  0.1404495    -0.18700644    0.2732884 0        0         1      0         \n",
       "4  0.6344959    -0.11581168    0.2741324 1        0         0      0         \n",
       "5 -1.0991931    -0.03247975   -0.5151724 0        1         0      0         \n",
       "6 -0.6573622    -0.07763347   -0.4598522 0        1         0      0         \n",
       "  ISLAND median_house_value\n",
       "1 0       56700            \n",
       "2 0      143400            \n",
       "3 0      128300            \n",
       "4 0      233200            \n",
       "5 0      110200            \n",
       "6 0      350900            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "\n",
    "housing.tidy = read_csv('housing.csv')\n",
    "\n",
    "housing.tidy = housing.tidy %>% \n",
    "  mutate(total_bedrooms = ifelse(is.na(total_bedrooms), \n",
    "                                 median(total_bedrooms, na.rm = T),\n",
    "                                 total_bedrooms),\n",
    "         mean_bedrooms = total_bedrooms/households,\n",
    "         mean_rooms = total_rooms/households) %>%\n",
    "  select(-c(total_rooms, total_bedrooms))\n",
    "\n",
    "\n",
    "categories = unique(housing.tidy$ocean_proximity) # all categories\n",
    "\n",
    "cat_housing.tidy = categories %>% # compare the full vector against each category consecutively\n",
    "  lapply(function(x) as.numeric(housing.tidy$ocean_proximity == x)) %>% # convert to numeric\n",
    "  do.call(\"cbind\", .) %>% as_tibble() # clean up\n",
    "colnames(cat_housing.tidy) = categories # make nice column names\n",
    "\n",
    "cleaned_housing.tidy = housing.tidy %>% \n",
    "  select(-c(ocean_proximity, median_house_value)) %>%\n",
    "  scale() %>% as_tibble() %>%\n",
    "  bind_cols(cat_housing.tidy) %>%\n",
    "  add_column(median_house_value = housing.tidy$median_house_value)\n",
    "\n",
    "set.seed(19) # Set a random seed so that same sample can be reproduced in future runs\n",
    "\n",
    "sample = sample.int(n = nrow(cleaned_housing.tidy), size = floor(.8*nrow(cleaned_housing.tidy)), replace = F)\n",
    "train = cleaned_housing.tidy[sample, ] #just the samples\n",
    "test  = cleaned_housing.tidy[-sample, ] #everything but the samples\n",
    "      \n",
    "head(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Last week's random forest model\n",
    "\n",
    "This is the model we made at the end of last week's session I'm running it again here so that we have a benchmark to for the other models we train today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "randomForest 4.6-12\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "Attaching package: ‘randomForest’\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    combine\n",
      "\n",
      "The following object is masked from ‘package:ggplot2’:\n",
      "\n",
      "    margin\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'call'</li>\n",
       "\t<li>'type'</li>\n",
       "\t<li>'predicted'</li>\n",
       "\t<li>'mse'</li>\n",
       "\t<li>'rsq'</li>\n",
       "\t<li>'oob.times'</li>\n",
       "\t<li>'importance'</li>\n",
       "\t<li>'importanceSD'</li>\n",
       "\t<li>'localImportance'</li>\n",
       "\t<li>'proximity'</li>\n",
       "\t<li>'ntree'</li>\n",
       "\t<li>'mtry'</li>\n",
       "\t<li>'forest'</li>\n",
       "\t<li>'coefs'</li>\n",
       "\t<li>'y'</li>\n",
       "\t<li>'test'</li>\n",
       "\t<li>'inbag'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'call'\n",
       "\\item 'type'\n",
       "\\item 'predicted'\n",
       "\\item 'mse'\n",
       "\\item 'rsq'\n",
       "\\item 'oob.times'\n",
       "\\item 'importance'\n",
       "\\item 'importanceSD'\n",
       "\\item 'localImportance'\n",
       "\\item 'proximity'\n",
       "\\item 'ntree'\n",
       "\\item 'mtry'\n",
       "\\item 'forest'\n",
       "\\item 'coefs'\n",
       "\\item 'y'\n",
       "\\item 'test'\n",
       "\\item 'inbag'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'call'\n",
       "2. 'type'\n",
       "3. 'predicted'\n",
       "4. 'mse'\n",
       "5. 'rsq'\n",
       "6. 'oob.times'\n",
       "7. 'importance'\n",
       "8. 'importanceSD'\n",
       "9. 'localImportance'\n",
       "10. 'proximity'\n",
       "11. 'ntree'\n",
       "12. 'mtry'\n",
       "13. 'forest'\n",
       "14. 'coefs'\n",
       "15. 'y'\n",
       "16. 'test'\n",
       "17. 'inbag'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"call\"            \"type\"            \"predicted\"       \"mse\"            \n",
       " [5] \"rsq\"             \"oob.times\"       \"importance\"      \"importanceSD\"   \n",
       " [9] \"localImportance\" \"proximity\"       \"ntree\"           \"mtry\"           \n",
       "[13] \"forest\"          \"coefs\"           \"y\"               \"test\"           \n",
       "[17] \"inbag\"          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>%IncMSE</th><th scope=col>IncNodePurity</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>NEAR BAY</th><td>4.450369e+08</td><td>1.391014e+12</td></tr>\n",
       "\t<tr><th scope=row>&lt;1H OCEAN</th><td>1.427100e+09</td><td>4.338351e+12</td></tr>\n",
       "\t<tr><th scope=row>INLAND</th><td>3.703060e+09</td><td>3.116839e+13</td></tr>\n",
       "\t<tr><th scope=row>NEAR OCEAN</th><td>4.262002e+08</td><td>2.119833e+12</td></tr>\n",
       "\t<tr><th scope=row>ISLAND</th><td>6.761432e+04</td><td>1.682004e+10</td></tr>\n",
       "\t<tr><th scope=row>longitude</th><td>6.686360e+09</td><td>2.580455e+13</td></tr>\n",
       "\t<tr><th scope=row>latitude</th><td>5.375308e+09</td><td>2.235183e+13</td></tr>\n",
       "\t<tr><th scope=row>housing_median_age</th><td>1.062666e+09</td><td>9.808021e+12</td></tr>\n",
       "\t<tr><th scope=row>population</th><td>1.081431e+09</td><td>7.441795e+12</td></tr>\n",
       "\t<tr><th scope=row>households</th><td>1.193537e+09</td><td>7.897400e+12</td></tr>\n",
       "\t<tr><th scope=row>median_income</th><td>8.325735e+09</td><td>7.167956e+13</td></tr>\n",
       "\t<tr><th scope=row>mean_bedrooms</th><td>4.083603e+08</td><td>7.597531e+12</td></tr>\n",
       "\t<tr><th scope=row>mean_rooms</th><td>1.890830e+09</td><td>2.155527e+13</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & \\%IncMSE & IncNodePurity\\\\\n",
       "\\hline\n",
       "\tNEAR BAY & 4.450369e+08 & 1.391014e+12\\\\\n",
       "\t<1H OCEAN & 1.427100e+09 & 4.338351e+12\\\\\n",
       "\tINLAND & 3.703060e+09 & 3.116839e+13\\\\\n",
       "\tNEAR OCEAN & 4.262002e+08 & 2.119833e+12\\\\\n",
       "\tISLAND & 6.761432e+04 & 1.682004e+10\\\\\n",
       "\tlongitude & 6.686360e+09 & 2.580455e+13\\\\\n",
       "\tlatitude & 5.375308e+09 & 2.235183e+13\\\\\n",
       "\thousing\\_median\\_age & 1.062666e+09 & 9.808021e+12\\\\\n",
       "\tpopulation & 1.081431e+09 & 7.441795e+12\\\\\n",
       "\thouseholds & 1.193537e+09 & 7.897400e+12\\\\\n",
       "\tmedian\\_income & 8.325735e+09 & 7.167956e+13\\\\\n",
       "\tmean\\_bedrooms & 4.083603e+08 & 7.597531e+12\\\\\n",
       "\tmean\\_rooms & 1.890830e+09 & 2.155527e+13\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | %IncMSE | IncNodePurity | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| NEAR BAY | 4.450369e+08 | 1.391014e+12 | \n",
       "| <1H OCEAN | 1.427100e+09 | 4.338351e+12 | \n",
       "| INLAND | 3.703060e+09 | 3.116839e+13 | \n",
       "| NEAR OCEAN | 4.262002e+08 | 2.119833e+12 | \n",
       "| ISLAND | 6.761432e+04 | 1.682004e+10 | \n",
       "| longitude | 6.686360e+09 | 2.580455e+13 | \n",
       "| latitude | 5.375308e+09 | 2.235183e+13 | \n",
       "| housing_median_age | 1.062666e+09 | 9.808021e+12 | \n",
       "| population | 1.081431e+09 | 7.441795e+12 | \n",
       "| households | 1.193537e+09 | 7.897400e+12 | \n",
       "| median_income | 8.325735e+09 | 7.167956e+13 | \n",
       "| mean_bedrooms | 4.083603e+08 | 7.597531e+12 | \n",
       "| mean_rooms | 1.890830e+09 | 2.155527e+13 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "                   %IncMSE      IncNodePurity\n",
       "NEAR BAY           4.450369e+08 1.391014e+12 \n",
       "<1H OCEAN          1.427100e+09 4.338351e+12 \n",
       "INLAND             3.703060e+09 3.116839e+13 \n",
       "NEAR OCEAN         4.262002e+08 2.119833e+12 \n",
       "ISLAND             6.761432e+04 1.682004e+10 \n",
       "longitude          6.686360e+09 2.580455e+13 \n",
       "latitude           5.375308e+09 2.235183e+13 \n",
       "housing_median_age 1.062666e+09 9.808021e+12 \n",
       "population         1.081431e+09 7.441795e+12 \n",
       "households         1.193537e+09 7.897400e+12 \n",
       "median_income      8.325735e+09 7.167956e+13 \n",
       "mean_bedrooms      4.083603e+08 7.597531e+12 \n",
       "mean_rooms         1.890830e+09 2.155527e+13 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>median_income</dt>\n",
       "\t\t<dd>8325735051.79272</dd>\n",
       "\t<dt>longitude</dt>\n",
       "\t\t<dd>6686359889.42546</dd>\n",
       "\t<dt>latitude</dt>\n",
       "\t\t<dd>5375307640.79577</dd>\n",
       "\t<dt>INLAND</dt>\n",
       "\t\t<dd>3703059838.7179</dd>\n",
       "\t<dt>mean_rooms</dt>\n",
       "\t\t<dd>1890829634.71922</dd>\n",
       "\t<dt>&lt;1H OCEAN</dt>\n",
       "\t\t<dd>1427100136.39297</dd>\n",
       "\t<dt>households</dt>\n",
       "\t\t<dd>1193537183.56301</dd>\n",
       "\t<dt>population</dt>\n",
       "\t\t<dd>1081430788.36861</dd>\n",
       "\t<dt>housing_median_age</dt>\n",
       "\t\t<dd>1062665767.43207</dd>\n",
       "\t<dt>NEAR BAY</dt>\n",
       "\t\t<dd>445036940.211241</dd>\n",
       "\t<dt>NEAR OCEAN</dt>\n",
       "\t\t<dd>426200169.288826</dd>\n",
       "\t<dt>mean_bedrooms</dt>\n",
       "\t\t<dd>408360297.039669</dd>\n",
       "\t<dt>ISLAND</dt>\n",
       "\t\t<dd>67614.3178751267</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[median\\textbackslash{}\\_income] 8325735051.79272\n",
       "\\item[longitude] 6686359889.42546\n",
       "\\item[latitude] 5375307640.79577\n",
       "\\item[INLAND] 3703059838.7179\n",
       "\\item[mean\\textbackslash{}\\_rooms] 1890829634.71922\n",
       "\\item[<1H OCEAN] 1427100136.39297\n",
       "\\item[households] 1193537183.56301\n",
       "\\item[population] 1081430788.36861\n",
       "\\item[housing\\textbackslash{}\\_median\\textbackslash{}\\_age] 1062665767.43207\n",
       "\\item[NEAR BAY] 445036940.211241\n",
       "\\item[NEAR OCEAN] 426200169.288826\n",
       "\\item[mean\\textbackslash{}\\_bedrooms] 408360297.039669\n",
       "\\item[ISLAND] 67614.3178751267\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "median_income\n",
       ":   8325735051.79272longitude\n",
       ":   6686359889.42546latitude\n",
       ":   5375307640.79577INLAND\n",
       ":   3703059838.7179mean_rooms\n",
       ":   1890829634.71922&amp;lt;1H OCEAN\n",
       ":   1427100136.39297households\n",
       ":   1193537183.56301population\n",
       ":   1081430788.36861housing_median_age\n",
       ":   1062665767.43207NEAR BAY\n",
       ":   445036940.211241NEAR OCEAN\n",
       ":   426200169.288826mean_bedrooms\n",
       ":   408360297.039669ISLAND\n",
       ":   67614.3178751267\n",
       "\n"
      ],
      "text/plain": [
       "     median_income          longitude           latitude             INLAND \n",
       "      8.325735e+09       6.686360e+09       5.375308e+09       3.703060e+09 \n",
       "        mean_rooms          <1H OCEAN         households         population \n",
       "      1.890830e+09       1.427100e+09       1.193537e+09       1.081431e+09 \n",
       "housing_median_age           NEAR BAY         NEAR OCEAN      mean_bedrooms \n",
       "      1.062666e+09       4.450369e+08       4.262002e+08       4.083603e+08 \n",
       "            ISLAND \n",
       "      6.761432e+04 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "48973.7871553834"
      ],
      "text/latex": [
       "48973.7871553834"
      ],
      "text/markdown": [
       "48973.7871553834"
      ],
      "text/plain": [
       "[1] 48973.79"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "48392.8697349542"
      ],
      "text/latex": [
       "48392.8697349542"
      ],
      "text/markdown": [
       "48392.8697349542"
      ],
      "text/plain": [
       "[1] 48392.87"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "########\n",
    "# Random Forest Model\n",
    "########\n",
    "library(randomForest)\n",
    "rf_model = randomForest(train_x, y = train_y , ntree = 500, importance = TRUE)\n",
    "\n",
    "names(rf_model) #these are all the different things you can call from the model.\n",
    "\n",
    "importance_dat = rf_model$importance\n",
    "importance_dat\n",
    "\n",
    "sorted_predictors = sort(importance_dat[,1], decreasing=TRUE)\n",
    "sorted_predictors\n",
    "\n",
    "oob_prediction = predict(rf_model) #leaving out a data source forces OOB predictions\n",
    "\n",
    "#you may have noticed that this is avaliable using the $mse in the model options.\n",
    "#but this way we learn stuff!\n",
    "train_mse = mean(as.numeric((oob_prediction - train_y)^2))\n",
    "oob_rmse = sqrt(train_mse)\n",
    "oob_rmse\n",
    "\n",
    "\n",
    "y_pred_rf = predict(rf_model , test_x)\n",
    "test_mse = mean(((y_pred_rf - test_y)^2))\n",
    "test_rmse = sqrt(test_mse)\n",
    "test_rmse # ~48620\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$48392 is the test error benchmark based off that random forest run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Gradient Boosting\n",
    "\n",
    "Gradient boosting is an ensemble supervised machine learning model that builds up the concept of the random forest algorithm we explored last week. Recall that for a random forest we spawned 500 decision trees and took the mean of their predictions to get a 'wisdom of the crowd' effect and arrive at a more accurate prediction than any one tree would provide.\n",
    "\n",
    "Here we use the Extreme Gradient Boosting library to implement this in R. Note the 'Extreme' in extreme gradient boosting refers to the computational efficiency. The algorithm could more accurately could be described as 'regularized gradient boosting'.\n",
    "\n",
    "###  Gradient Boosting - Algorithm details\n",
    "\n",
    "Extreme gradient boosting also builds a forest of trees, but does so in an additive manner. The algorithm iteratively builds trees that minimize the error, and thereby descends towards an optimal set of predictive trees. Already learned trees are kept, and new trees are added one after another to minimize the objective function (error in predictions). The trees are grown sequentially: each tree is grown using information from previously grown trees. Each tree is fit on a modified version of the original data set based on the previous trees built.\n",
    "\n",
    "The trees are accompanied by a regularization paramater to avoid overfit.  \n",
    "\n",
    "One difference between boosting and random forests: in boosting, because the growth of a particular \n",
    "tree takes into account the other trees that have already been grown, smaller trees are typically sufficient (less splits and depth). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-rmse:171405.234375\ttest-rmse:173186.609375 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 50 rounds.\n",
      "\n",
      "[2]\ttrain-rmse:126655.414062\ttest-rmse:129169.820312 \n",
      "[3]\ttrain-rmse:96442.976562\ttest-rmse:100198.515625 \n",
      "[4]\ttrain-rmse:76786.750000\ttest-rmse:81659.406250 \n",
      "[5]\ttrain-rmse:64022.230469\ttest-rmse:70192.492188 \n",
      "[6]\ttrain-rmse:55857.519531\ttest-rmse:63414.535156 \n",
      "[7]\ttrain-rmse:50131.742188\ttest-rmse:58767.277344 \n",
      "[8]\ttrain-rmse:46460.761719\ttest-rmse:56293.750000 \n",
      "[9]\ttrain-rmse:43681.136719\ttest-rmse:54576.554688 \n",
      "[10]\ttrain-rmse:41670.960938\ttest-rmse:53205.472656 \n",
      "[11]\ttrain-rmse:40081.183594\ttest-rmse:52478.437500 \n",
      "[12]\ttrain-rmse:38772.078125\ttest-rmse:51718.906250 \n",
      "[13]\ttrain-rmse:37653.445312\ttest-rmse:51174.562500 \n",
      "[14]\ttrain-rmse:36709.722656\ttest-rmse:50757.242188 \n",
      "[15]\ttrain-rmse:35945.863281\ttest-rmse:50291.871094 \n",
      "[16]\ttrain-rmse:35438.671875\ttest-rmse:50195.484375 \n",
      "[17]\ttrain-rmse:34626.863281\ttest-rmse:49946.457031 \n",
      "[18]\ttrain-rmse:33869.035156\ttest-rmse:49355.011719 \n",
      "[19]\ttrain-rmse:33223.679688\ttest-rmse:49304.156250 \n",
      "[20]\ttrain-rmse:32905.074219\ttest-rmse:49202.875000 \n",
      "[21]\ttrain-rmse:32372.568359\ttest-rmse:49059.015625 \n",
      "[22]\ttrain-rmse:31666.564453\ttest-rmse:49023.015625 \n",
      "[23]\ttrain-rmse:31386.929688\ttest-rmse:48957.046875 \n",
      "[24]\ttrain-rmse:30832.781250\ttest-rmse:48816.902344 \n",
      "[25]\ttrain-rmse:30282.021484\ttest-rmse:48738.679688 \n",
      "[26]\ttrain-rmse:29886.916016\ttest-rmse:48649.777344 \n",
      "[27]\ttrain-rmse:29005.435547\ttest-rmse:48427.425781 \n",
      "[28]\ttrain-rmse:28590.333984\ttest-rmse:48443.882812 \n",
      "[29]\ttrain-rmse:28435.058594\ttest-rmse:48388.351562 \n",
      "[30]\ttrain-rmse:28129.888672\ttest-rmse:48376.511719 \n",
      "[31]\ttrain-rmse:27860.167969\ttest-rmse:48399.742188 \n",
      "[32]\ttrain-rmse:27417.722656\ttest-rmse:48390.902344 \n",
      "[33]\ttrain-rmse:26910.509766\ttest-rmse:48319.710938 \n",
      "[34]\ttrain-rmse:26525.353516\ttest-rmse:48283.984375 \n",
      "[35]\ttrain-rmse:26011.273438\ttest-rmse:48022.480469 \n",
      "[36]\ttrain-rmse:25778.001953\ttest-rmse:48001.406250 \n",
      "[37]\ttrain-rmse:25461.269531\ttest-rmse:48022.644531 \n",
      "[38]\ttrain-rmse:25239.115234\ttest-rmse:47956.375000 \n",
      "[39]\ttrain-rmse:24864.669922\ttest-rmse:47887.632812 \n",
      "[40]\ttrain-rmse:24572.902344\ttest-rmse:47863.503906 \n",
      "[41]\ttrain-rmse:24393.386719\ttest-rmse:47885.375000 \n",
      "[42]\ttrain-rmse:24054.414062\ttest-rmse:47802.382812 \n",
      "[43]\ttrain-rmse:23860.767578\ttest-rmse:47799.707031 \n",
      "[44]\ttrain-rmse:23651.214844\ttest-rmse:47785.679688 \n",
      "[45]\ttrain-rmse:23524.472656\ttest-rmse:47791.894531 \n",
      "[46]\ttrain-rmse:23298.683594\ttest-rmse:47830.031250 \n",
      "[47]\ttrain-rmse:22866.937500\ttest-rmse:47739.593750 \n",
      "[48]\ttrain-rmse:22763.912109\ttest-rmse:47773.503906 \n",
      "[49]\ttrain-rmse:22646.037109\ttest-rmse:47778.355469 \n",
      "[50]\ttrain-rmse:22372.734375\ttest-rmse:47782.691406 \n",
      "[51]\ttrain-rmse:22239.695312\ttest-rmse:47789.132812 \n",
      "[52]\ttrain-rmse:22141.908203\ttest-rmse:47773.460938 \n",
      "[53]\ttrain-rmse:22014.251953\ttest-rmse:47789.722656 \n",
      "[54]\ttrain-rmse:21895.628906\ttest-rmse:47796.968750 \n",
      "[55]\ttrain-rmse:21657.162109\ttest-rmse:47747.050781 \n",
      "[56]\ttrain-rmse:21571.712891\ttest-rmse:47723.835938 \n",
      "[57]\ttrain-rmse:21385.740234\ttest-rmse:47738.277344 \n",
      "[58]\ttrain-rmse:21193.324219\ttest-rmse:47759.441406 \n",
      "[59]\ttrain-rmse:20994.201172\ttest-rmse:47769.265625 \n",
      "[60]\ttrain-rmse:20958.703125\ttest-rmse:47756.937500 \n",
      "[61]\ttrain-rmse:20752.375000\ttest-rmse:47764.039062 \n",
      "[62]\ttrain-rmse:20680.550781\ttest-rmse:47758.011719 \n",
      "[63]\ttrain-rmse:20567.132812\ttest-rmse:47743.968750 \n",
      "[64]\ttrain-rmse:20514.886719\ttest-rmse:47740.730469 \n",
      "[65]\ttrain-rmse:20320.652344\ttest-rmse:47764.601562 \n",
      "[66]\ttrain-rmse:20182.564453\ttest-rmse:47774.683594 \n",
      "[67]\ttrain-rmse:20117.828125\ttest-rmse:47780.882812 \n",
      "[68]\ttrain-rmse:19949.353516\ttest-rmse:47806.039062 \n",
      "[69]\ttrain-rmse:19776.464844\ttest-rmse:47778.058594 \n",
      "[70]\ttrain-rmse:19751.880859\ttest-rmse:47752.632812 \n",
      "[71]\ttrain-rmse:19564.796875\ttest-rmse:47746.855469 \n",
      "[72]\ttrain-rmse:19497.765625\ttest-rmse:47759.941406 \n",
      "[73]\ttrain-rmse:19418.083984\ttest-rmse:47752.867188 \n",
      "[74]\ttrain-rmse:19288.673828\ttest-rmse:47767.101562 \n",
      "[75]\ttrain-rmse:19122.927734\ttest-rmse:47768.851562 \n",
      "[76]\ttrain-rmse:18850.068359\ttest-rmse:47835.789062 \n",
      "[77]\ttrain-rmse:18741.849609\ttest-rmse:47843.453125 \n",
      "[78]\ttrain-rmse:18526.640625\ttest-rmse:47860.578125 \n",
      "[79]\ttrain-rmse:18486.335938\ttest-rmse:47861.585938 \n",
      "[80]\ttrain-rmse:18394.787109\ttest-rmse:47832.167969 \n",
      "[81]\ttrain-rmse:18281.445312\ttest-rmse:47851.167969 \n",
      "[82]\ttrain-rmse:17979.539062\ttest-rmse:47829.855469 \n",
      "[83]\ttrain-rmse:17914.548828\ttest-rmse:47844.824219 \n",
      "[84]\ttrain-rmse:17774.501953\ttest-rmse:47853.578125 \n",
      "[85]\ttrain-rmse:17719.486328\ttest-rmse:47877.273438 \n",
      "[86]\ttrain-rmse:17592.980469\ttest-rmse:47883.550781 \n",
      "[87]\ttrain-rmse:17558.435547\ttest-rmse:47868.367188 \n",
      "[88]\ttrain-rmse:17382.988281\ttest-rmse:47825.703125 \n",
      "[89]\ttrain-rmse:17228.353516\ttest-rmse:47860.203125 \n",
      "[90]\ttrain-rmse:17148.488281\ttest-rmse:47872.394531 \n",
      "[91]\ttrain-rmse:17105.558594\ttest-rmse:47860.261719 \n",
      "[92]\ttrain-rmse:16953.853516\ttest-rmse:47876.179688 \n",
      "[93]\ttrain-rmse:16871.755859\ttest-rmse:47861.667969 \n",
      "[94]\ttrain-rmse:16807.742188\ttest-rmse:47848.078125 \n",
      "[95]\ttrain-rmse:16627.542969\ttest-rmse:47815.453125 \n",
      "[96]\ttrain-rmse:16528.035156\ttest-rmse:47815.347656 \n",
      "[97]\ttrain-rmse:16396.185547\ttest-rmse:47848.863281 \n",
      "[98]\ttrain-rmse:16275.927734\ttest-rmse:47827.125000 \n",
      "[99]\ttrain-rmse:16110.612305\ttest-rmse:47772.335938 \n",
      "[100]\ttrain-rmse:15998.253906\ttest-rmse:47784.789062 \n",
      "[101]\ttrain-rmse:15978.763672\ttest-rmse:47781.835938 \n",
      "[102]\ttrain-rmse:15851.356445\ttest-rmse:47776.453125 \n",
      "[103]\ttrain-rmse:15816.543945\ttest-rmse:47776.269531 \n",
      "[104]\ttrain-rmse:15762.886719\ttest-rmse:47775.089844 \n",
      "[105]\ttrain-rmse:15717.681641\ttest-rmse:47783.968750 \n",
      "[106]\ttrain-rmse:15597.377930\ttest-rmse:47791.609375 \n",
      "Stopping. Best iteration:\n",
      "[56]\ttrain-rmse:21571.712891\ttest-rmse:47723.835938\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######\n",
    "# XG Boost\n",
    "######\n",
    "# see the docs: http://cran.fhcrc.org/web/packages/xgboost/vignettes/xgboost.pdf\n",
    "library(xgboost)\n",
    "\n",
    "#put into the xgb matrix format\n",
    "dtrain = xgb.DMatrix(data =  as.matrix(train_x), label = train_y )\n",
    "dtest = xgb.DMatrix(data =  as.matrix(test_x), label = test_y)\n",
    "\n",
    "# these are the datasets the rmse is evaluated for at each iteration\n",
    "watchlist = list(train=dtrain, test=dtest)\n",
    "\n",
    "# try 1 - off a set of paramaters I know work pretty well for most stuff\n",
    "\n",
    "bst = xgb.train(data = dtrain, \n",
    "                max.depth = 8, \n",
    "                eta = 0.3, \n",
    "                nthread = 2, \n",
    "                nround = 1000, \n",
    "                watchlist = watchlist, \n",
    "                objective = \"reg:linear\", \n",
    "                early_stopping_rounds = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our first run there gets a rmse of $47723, an improvement over our benchmark model. That isn't the end of the story though, we can try to squeak out further improvements through 'hyperparameter tuning'. A hyperparameter is one of the mutable options that we pass to the algorithm along with our data.\n",
    "\n",
    "## Tuning the algorithm - hyperparameters for xgboost\n",
    "\n",
    "Boosting has 3 tuning paramaters that we can focus on\n",
    "\n",
    "1. The number of trees. Here we use a good trick, instead of specifying an exact number, we give the algorithm a big number (nround = 10000) and the param (early_stopping_rounds = 50). This effectively means: 'keep iteratively growing trees until you have 10,000 of them, or stop early if the scores haven't improved for the last 50 rounds'.\n",
    "2. The shrinkage parameter λ (eta in the params), a small positive number. This controls the rate at which boosting learns. Typical values are 0.01 or 0.001, and the right choice can depend on the problem. Very small λ can require using a very large value of B in order to achieve good performance.\n",
    "3. The number of splits in each tree, which controls the complexity of the boosted ensemble (controlled with max.depth).\n",
    "\n",
    "### Here we try a 'slower learning' model. The up and down weights for each iteration are smaller we also use more iterations to account for the fact that the model will take longer to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst_slow = xgb.train(data = dtrain, \n",
    "                        max.depth=8, \n",
    "                        eta = 0.01, \n",
    "                        nthread = 2, \n",
    "                        nround = 10000, \n",
    "                        watchlist = watchlist, \n",
    "                        objective = \"reg:linear\", \n",
    "                        early_stopping_rounds = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_benchmark = 48392\n",
    "\n",
    "bst_slow$best_score / rf_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rmse: 45225.968750 \n",
    "\n",
    "#last week: $48620 with random forest\n",
    "# an improvement of ~$3400 in average error. Wait! What we have done here is fit to the training set (leading to model overfit). Need to work with a validation set, then only at the end evaluate the model performance against the test set.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems to address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "####\n",
    "# Proper use - validation set\n",
    "####\n",
    "\n",
    "#validation set - Another subset of our data that is witheld from the training algorithm, but compared against at each iteration to see how\n",
    "\n",
    "#make validation set\n",
    "\n",
    "set.seed(19) # Set a random seed so that same sample can be reproduced in future runs\n",
    "\n",
    "sample = sample.int(n = nrow(train), size = floor(.8*nrow(train)), replace = F)\n",
    "\n",
    "train_t = train[sample, ] #just the samples\n",
    "valid  = train[-sample, ] #everything but the samples\n",
    "\n",
    "train_y = train_t[,'median_house_value']\n",
    "train_x = train_t[, names(train) !='median_house_value']\n",
    "\n",
    "valid_y = valid[,'median_house_value']\n",
    "valid_x = valid[, names(test) !='median_house_value']\n",
    "\n",
    "gb_train = xgb.DMatrix(data = as.matrix(train_x), label = train_y )\n",
    "gb_valid = xgb.DMatrix(data = as.matrix(valid_x), label = valid_y)\n",
    "\n",
    "# train xgb, evaluating against the validation\n",
    "watchlist = list(train = gb_train, valid = gb_valid)\n",
    "\n",
    "bst_slow = xgb.train(data= gb_train, \n",
    "                        max.depth = 10, \n",
    "                        eta = 0.01, \n",
    "                        nthread = 2, \n",
    "                        nround = 10000, \n",
    "                        watchlist = watchlist, \n",
    "                        objective = \"reg:linear\", \n",
    "                        early_stopping_rounds = 50)\n",
    "\n",
    "# error, need the matrix format\n",
    "y_hat = predict(bst_slow, test_x)\n",
    "\n",
    "# recall we ran the following to get the test data in the right format:\n",
    "# dtest = xgb.DMatrix(data =  as.matrix(test_x), label = test_y)\n",
    "# here I have it with the label taken off, just to remind us its external data xgb would ignore the label though during predictions\n",
    "dtest = xgb.DMatrix(data =  as.matrix(test_x))\n",
    "\n",
    "#test the model on truly external data\n",
    "\n",
    "y_hat_valid = predict(bst_slow, dtest)\n",
    "\n",
    "test_mse = mean(((y_hat_valid - test_y)^2))\n",
    "test_rmse = sqrt(test_mse)\n",
    "test_rmse \n",
    "# ~47507.09 This is higher then on the first run through, but we can be confident that the improved score is not due to overfit thanks to our use of a validation set! point out that this is evidence of how a lower rmse isn't necessarily better, as we now have more confidence in external predictions.2.3% improvement over a basic random forest... is it worth the effort? The answer to this question always depends on the purpose of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "###\n",
    "# Grid search first principles \n",
    "###\n",
    "\n",
    "max.depths = c(3, 5, 7, 9)\n",
    "etas = c(0.01, 0.001, 0.0001)\n",
    "\n",
    "best_params = 0\n",
    "best_score = 0\n",
    "\n",
    "count = 1\n",
    "for( depth in max.depths ){\n",
    "    for( num in etas){\n",
    "\n",
    "        bst_grid = xgb.train(data = gb_train, \n",
    "                                max.depth = depth, \n",
    "                                eta=num, \n",
    "                                nthread = 2, \n",
    "                                nround = 10000, \n",
    "                                watchlist = watchlist, \n",
    "                                objective = \"reg:linear\", \n",
    "                                early_stopping_rounds = 50, \n",
    "                                verbose=0)\n",
    "\n",
    "        if(count == 1){\n",
    "            best_params = bst_grid$params\n",
    "            best_score = bst_grid$best_score\n",
    "            count = count + 1\n",
    "            }\n",
    "        else if( bst_grid$best_score < best_score){\n",
    "            best_params = bst_grid$params\n",
    "            best_score = bst_grid$best_score\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "best_params\n",
    "best_score\n",
    "#valid-rmse: 47033.28\n",
    "\n",
    "# max_depth of 9, eta of 0.01\n",
    "bst_tuned = xgb.train( data = gb_train, \n",
    "                        max.depth = 9, \n",
    "                        eta = 0.01, \n",
    "                        nthread = 2, \n",
    "                        nround = 10000, \n",
    "                        watchlist = watchlist, \n",
    "                        objective = \"reg:linear\", \n",
    "                        early_stopping_rounds = 50)\n",
    "\n",
    "y_hat_xgb_grid = predict(bst_tuned, dtest)\n",
    "\n",
    "test_mse = mean(((y_hat_xgb_grid - test_y)^2))\n",
    "test_rmse = sqrt(test_mse)\n",
    "test_rmse # test-rmse: 46675\n",
    "# By tuning the hyperparamaters we have moved to a 4% improvement over random forest\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#######\n",
    "# tweak the hyperparamaters using a grid search\n",
    "# The caret package (short for classification and regression training)\n",
    "######\n",
    "\n",
    "library(caret) \n",
    "\n",
    "# look up the model we are running to see the paramaters\n",
    "modelLookup(\"xgbLinear\")\n",
    " \n",
    "# set up all the pairwise combinations\n",
    "\n",
    "xgb_grid_1 = expand.grid(nrounds = c(1000,2000,3000,4000) ,\n",
    "                            eta = c(0.01, 0.001, 0.0001),\n",
    "                            lambda = 1,\n",
    "                            alpha = 0)\n",
    "xgb_grid_1\n",
    "\n",
    "\n",
    "#here we do one better then a validation set, we use cross validation to \n",
    "#expand the amount of info we have!\n",
    "xgb_trcontrol_1 = trainControl(method = \"cv\",\n",
    "                                number = 5,\n",
    "                                verboseIter = TRUE,\n",
    "                                returnData = FALSE,\n",
    "                                returnResamp = \"all\", \n",
    "                                allowParallel = TRUE)\n",
    "\n",
    "\n",
    "######\n",
    "#below a grid-search, cross-validation xgboost model in caret\n",
    "######\n",
    "# train the model for each parameter combination in the grid, using CV to evaluate on multiple folds. Make sure your laptop is plugged in or RIP battery.\n",
    "\n",
    "# note how this is now the caret train function\n",
    "?train\n",
    "\n",
    "xgb_train_1 = train(x = as.matrix(train_x),\n",
    "\t\t\t\t\ty = train_y,\n",
    "\t\t\t\t\ttrControl = xgb_trcontrol_1,\n",
    "\t\t\t\t\ttuneGrid = xgb_grid_1,\n",
    "\t\t\t\t\tmethod = \"xgbLinear\",\n",
    "\t\t\t\t\tmax.depth = 5)\n",
    "\n",
    "names(xgb_train_1)\n",
    "xgb_train_1$bestTune\n",
    "xgb_train_1$method\n",
    "summary(xgb_train_1)\n",
    "\n",
    "\n",
    "#alternatively, you can 'narrow in' on the best paramaters by taking a range of options around the best values found and seeing if high resolution tweaks can provide even further improvements.\n",
    "\n",
    "xgb_cv_yhat = predict(xgb_train_1 , as.matrix(test_x))\n",
    "\n",
    "\n",
    "test_mse = mean(((xgb_cv_yhat - test_y)^2))\n",
    "test_rmse = sqrt(test_mse)\n",
    "test_rmse # 46641... pretty close to the 'by hand' grid search!\n",
    "\n",
    "#Cam's hypothesis - not using 'early stopping rounds' here so the model isn't cutting out at the exact best point. re-running this with a validation setup as opposed to a cv setup would allow us to implement a grid search efficiently and wind up with the best hyperparamaters. I shall leave this as a follow up exercise for the curious.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# Ensemble the models together, \n",
    "# strategy for when accuracy is more important then knowing the best predictors\n",
    "########\n",
    "\n",
    "\n",
    "y_pred_rf #random forest\n",
    "y_hat_valid #xgBoost with validation\n",
    "y_hat_xgb_grid #xgBoost grid search\n",
    "xgb_cv_yhat #xgBoost caret cross validation\n",
    "\n",
    "length(y_hat_xgb_grid)\n",
    "\n",
    "\n",
    "blend_pred = (y_hat * .25) + (y_pred_rf * .25) + (xgb_cv_yhat * .25) + (y_hat_xgb_grid * .25)\n",
    "length(blend_pred)\n",
    "\n",
    "length(blend_pred) == length(y_hat_xgb_grid)\n",
    "\n",
    "blend_test_mse = mean(((blend_pred - test_y)^2))\n",
    "blend_test_rmse = sqrt(blend_test_mse)\n",
    "blend_test_rmse # 45205 by averaging just 4 predictors we have dropped the rmse a few percent lower then the best scoring of the 4 models. This does come at a cost though, we now can't make accurate inferrences about the best predictors!\n",
    "\n",
    "#next step - you can grid search the weights of the ensemble to try and drop the rmse further!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
