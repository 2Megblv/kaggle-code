{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in R: part 2\n",
    "\n",
    "## This week focuses on improvement. We will our data cleaning code more efficient and then use some tuning methods to improve the predictive ability of models we build.\n",
    "\n",
    "\n",
    "## Step 1a. Cleaning and formatting the data (from last week)\n",
    "\n",
    "Below is the code from the loading, cleaning and train/test split sections we went over last week. This is all we requre to get the data into the format needed to being training some machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>NEAR BAY</th><th scope=col>&lt;1H OCEAN</th><th scope=col>INLAND</th><th scope=col>NEAR OCEAN</th><th scope=col>ISLAND</th><th scope=col>longitude</th><th scope=col>latitude</th><th scope=col>housing_median_age</th><th scope=col>population</th><th scope=col>households</th><th scope=col>median_income</th><th scope=col>mean_bedrooms</th><th scope=col>mean_rooms</th><th scope=col>median_house_value</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>2418</th><td>0          </td><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td> 0.06473791</td><td> 0.4485767 </td><td>-0.05081113</td><td>-0.08342596</td><td>-0.50882695</td><td>-1.2394168 </td><td>-0.03648780</td><td>-0.4145713 </td><td> 56700     </td></tr>\n",
       "\t<tr><th scope=row>9990</th><td>0          </td><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td>-0.74882545</td><td> 1.6471053 </td><td>-1.08374113</td><td> 1.39212008</td><td> 2.14071836</td><td>-0.7358959 </td><td>-0.19291092</td><td>-0.1004065 </td><td>143400     </td></tr>\n",
       "\t<tr><th scope=row>13440</th><td>0          </td><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td> 1.07295753</td><td>-0.7218613 </td><td>-0.05081113</td><td> 0.28656434</td><td> 0.06136148</td><td> 0.1404495 </td><td>-0.18700644</td><td> 0.2732884 </td><td>128300     </td></tr>\n",
       "\t<tr><th scope=row>1412</th><td>1          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>-1.25293526</td><td> 1.0759315 </td><td> 0.50538194</td><td> 0.35897294</td><td> 0.42492199</td><td> 0.6344959 </td><td>-0.11581168</td><td> 0.2741324 </td><td>233200     </td></tr>\n",
       "\t<tr><th scope=row>7539</th><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td>0          </td><td> 0.67865382</td><td>-0.8061329 </td><td>-0.20972344</td><td> 1.03802435</td><td> 0.21829408</td><td>-1.0991931 </td><td>-0.03247975</td><td>-0.5151724 </td><td>110200     </td></tr>\n",
       "\t<tr><th scope=row>4621</th><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td>0          </td><td> 0.62874196</td><td>-0.7265431 </td><td> 1.61776810</td><td> 0.10024464</td><td> 0.24706505</td><td>-0.6573622 </td><td>-0.07763347</td><td>-0.4598522 </td><td>350900     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllll}\n",
       "  & NEAR BAY & <1H OCEAN & INLAND & NEAR OCEAN & ISLAND & longitude & latitude & housing\\_median\\_age & population & households & median\\_income & mean\\_bedrooms & mean\\_rooms & median\\_house\\_value\\\\\n",
       "\\hline\n",
       "\t2418 & 0           & 0           & 1           & 0           & 0           &  0.06473791 &  0.4485767  & -0.05081113 & -0.08342596 & -0.50882695 & -1.2394168  & -0.03648780 & -0.4145713  &  56700     \\\\\n",
       "\t9990 & 0           & 0           & 1           & 0           & 0           & -0.74882545 &  1.6471053  & -1.08374113 &  1.39212008 &  2.14071836 & -0.7358959  & -0.19291092 & -0.1004065  & 143400     \\\\\n",
       "\t13440 & 0           & 0           & 1           & 0           & 0           &  1.07295753 & -0.7218613  & -0.05081113 &  0.28656434 &  0.06136148 &  0.1404495  & -0.18700644 &  0.2732884  & 128300     \\\\\n",
       "\t1412 & 1           & 0           & 0           & 0           & 0           & -1.25293526 &  1.0759315  &  0.50538194 &  0.35897294 &  0.42492199 &  0.6344959  & -0.11581168 &  0.2741324  & 233200     \\\\\n",
       "\t7539 & 0           & 1           & 0           & 0           & 0           &  0.67865382 & -0.8061329  & -0.20972344 &  1.03802435 &  0.21829408 & -1.0991931  & -0.03247975 & -0.5151724  & 110200     \\\\\n",
       "\t4621 & 0           & 1           & 0           & 0           & 0           &  0.62874196 & -0.7265431  &  1.61776810 &  0.10024464 &  0.24706505 & -0.6573622  & -0.07763347 & -0.4598522  & 350900     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | NEAR BAY | <1H OCEAN | INLAND | NEAR OCEAN | ISLAND | longitude | latitude | housing_median_age | population | households | median_income | mean_bedrooms | mean_rooms | median_house_value | \n",
       "|---|---|---|---|---|---|\n",
       "| 2418 | 0           | 0           | 1           | 0           | 0           |  0.06473791 |  0.4485767  | -0.05081113 | -0.08342596 | -0.50882695 | -1.2394168  | -0.03648780 | -0.4145713  |  56700      | \n",
       "| 9990 | 0           | 0           | 1           | 0           | 0           | -0.74882545 |  1.6471053  | -1.08374113 |  1.39212008 |  2.14071836 | -0.7358959  | -0.19291092 | -0.1004065  | 143400      | \n",
       "| 13440 | 0           | 0           | 1           | 0           | 0           |  1.07295753 | -0.7218613  | -0.05081113 |  0.28656434 |  0.06136148 |  0.1404495  | -0.18700644 |  0.2732884  | 128300      | \n",
       "| 1412 | 1           | 0           | 0           | 0           | 0           | -1.25293526 |  1.0759315  |  0.50538194 |  0.35897294 |  0.42492199 |  0.6344959  | -0.11581168 |  0.2741324  | 233200      | \n",
       "| 7539 | 0           | 1           | 0           | 0           | 0           |  0.67865382 | -0.8061329  | -0.20972344 |  1.03802435 |  0.21829408 | -1.0991931  | -0.03247975 | -0.5151724  | 110200      | \n",
       "| 4621 | 0           | 1           | 0           | 0           | 0           |  0.62874196 | -0.7265431  |  1.61776810 |  0.10024464 |  0.24706505 | -0.6573622  | -0.07763347 | -0.4598522  | 350900      | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "      NEAR BAY <1H OCEAN INLAND NEAR OCEAN ISLAND longitude   latitude  \n",
       "2418  0        0         1      0          0       0.06473791  0.4485767\n",
       "9990  0        0         1      0          0      -0.74882545  1.6471053\n",
       "13440 0        0         1      0          0       1.07295753 -0.7218613\n",
       "1412  1        0         0      0          0      -1.25293526  1.0759315\n",
       "7539  0        1         0      0          0       0.67865382 -0.8061329\n",
       "4621  0        1         0      0          0       0.62874196 -0.7265431\n",
       "      housing_median_age population  households  median_income mean_bedrooms\n",
       "2418  -0.05081113        -0.08342596 -0.50882695 -1.2394168    -0.03648780  \n",
       "9990  -1.08374113         1.39212008  2.14071836 -0.7358959    -0.19291092  \n",
       "13440 -0.05081113         0.28656434  0.06136148  0.1404495    -0.18700644  \n",
       "1412   0.50538194         0.35897294  0.42492199  0.6344959    -0.11581168  \n",
       "7539  -0.20972344         1.03802435  0.21829408 -1.0991931    -0.03247975  \n",
       "4621   1.61776810         0.10024464  0.24706505 -0.6573622    -0.07763347  \n",
       "      mean_rooms median_house_value\n",
       "2418  -0.4145713  56700            \n",
       "9990  -0.1004065 143400            \n",
       "13440  0.2732884 128300            \n",
       "1412   0.2741324 233200            \n",
       "7539  -0.5151724 110200            \n",
       "4621  -0.4598522 350900            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "\n",
    "housing = read.csv('./housing.csv')\n",
    "\n",
    "housing$total_bedrooms[is.na(housing$total_bedrooms)] = median(housing$total_bedrooms , na.rm = TRUE)\n",
    "\n",
    "housing$mean_bedrooms = housing$total_bedrooms/housing$households\n",
    "housing$mean_rooms = housing$total_rooms/housing$households\n",
    "\n",
    "drops = c('total_bedrooms', 'total_rooms')\n",
    "\n",
    "housing = housing[ , !(names(housing) %in% drops)]\n",
    "\n",
    "categories = unique(housing$ocean_proximity)\n",
    "#split the categories off\n",
    "cat_housing = data.frame(ocean_proximity = housing$ocean_proximity)\n",
    "\n",
    "for(cat in categories){\n",
    "    cat_housing[,cat] = rep(0, times= nrow(cat_housing))\n",
    "}\n",
    "\n",
    "for(i in 1:length(cat_housing$ocean_proximity)){\n",
    "    cat = as.character(cat_housing$ocean_proximity[i])\n",
    "    cat_housing[,cat][i] = 1\n",
    "}\n",
    "\n",
    "cat_columns = names(cat_housing)\n",
    "keep_columns = cat_columns[cat_columns != 'ocean_proximity']\n",
    "cat_housing = select(cat_housing,one_of(keep_columns))\n",
    "drops = c('ocean_proximity','median_house_value')\n",
    "housing_num =  housing[ , !(names(housing) %in% drops)]\n",
    "\n",
    "\n",
    "scaled_housing_num = scale(housing_num)\n",
    "\n",
    "cleaned_housing = cbind(cat_housing, scaled_housing_num, median_house_value=housing$median_house_value)\n",
    "\n",
    "\n",
    "set.seed(19) # Set a random seed so that same sample can be reproduced in future runs\n",
    "\n",
    "sample = sample.int(n = nrow(cleaned_housing), size = floor(.8*nrow(cleaned_housing)), replace = F)\n",
    "train = cleaned_housing[sample, ] #just the samples\n",
    "test  = cleaned_housing[-sample, ] #everything but the samples\n",
    "\n",
    "\n",
    "train_y = train[,'median_house_value']\n",
    "train_x = train[, names(train) !='median_house_value']\n",
    "\n",
    "test_y = test[,'median_house_value']\n",
    "test_x = test[, names(test) !='median_house_value']\n",
    "\n",
    "head(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1b. Cleaning - The tidyverse way! \n",
    "\n",
    "The code below does the same thing as the code above, but employs the tidyverse. I've pulled out the bare bones parts of Karl's 'Housing_R_tidy.r' script needed to get the data to where we want it (i.e. removed all the graphs head commands etc. Go to his original script to see the notes and visual blandishments).\n",
    "\n",
    "I like this code more then my original version because:\n",
    "1. It is easy to follow the workflow. magrittr makes it easy to see when one cleaning task ends and the next begins.\n",
    "2. It is more concise.\n",
    "3. The use of comments(#) after the pipes(%>%) looks professional and also makes the code more readable. Being able to share your code with others and have them understand it is very important!\n",
    "4. It runs faster.\n",
    "\n",
    "Note: in the tibble docs the function is listed as: as_tibble() not as.tibble() as first written. Oddly as.tibble() worked in my normal R deployment, but threw an error in the jupyter notebook :\\ This confused me and I don't know what to make of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsed with column specification:\n",
      "cols(\n",
      "  longitude = col_double(),\n",
      "  latitude = col_double(),\n",
      "  housing_median_age = col_double(),\n",
      "  total_rooms = col_double(),\n",
      "  total_bedrooms = col_double(),\n",
      "  population = col_double(),\n",
      "  households = col_double(),\n",
      "  median_income = col_double(),\n",
      "  median_house_value = col_double(),\n",
      "  ocean_proximity = col_character()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>longitude</th><th scope=col>latitude</th><th scope=col>housing_median_age</th><th scope=col>population</th><th scope=col>households</th><th scope=col>median_income</th><th scope=col>mean_bedrooms</th><th scope=col>mean_rooms</th><th scope=col>NEAR BAY</th><th scope=col>&lt;1H OCEAN</th><th scope=col>INLAND</th><th scope=col>NEAR OCEAN</th><th scope=col>ISLAND</th><th scope=col>median_house_value</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 0.06473791</td><td> 0.4485767 </td><td>-0.05081113</td><td>-0.08342596</td><td>-0.50882695</td><td>-1.2394168 </td><td>-0.03648780</td><td>-0.4145713 </td><td>0          </td><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td> 56700     </td></tr>\n",
       "\t<tr><td>-0.74882545</td><td> 1.6471053 </td><td>-1.08374113</td><td> 1.39212008</td><td> 2.14071836</td><td>-0.7358959 </td><td>-0.19291092</td><td>-0.1004065 </td><td>0          </td><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td>143400     </td></tr>\n",
       "\t<tr><td> 1.07295753</td><td>-0.7218613 </td><td>-0.05081113</td><td> 0.28656434</td><td> 0.06136148</td><td> 0.1404495 </td><td>-0.18700644</td><td> 0.2732884 </td><td>0          </td><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td>128300     </td></tr>\n",
       "\t<tr><td>-1.25293526</td><td> 1.0759315 </td><td> 0.50538194</td><td> 0.35897294</td><td> 0.42492199</td><td> 0.6344959 </td><td>-0.11581168</td><td> 0.2741324 </td><td>1          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>233200     </td></tr>\n",
       "\t<tr><td> 0.67865382</td><td>-0.8061329 </td><td>-0.20972344</td><td> 1.03802435</td><td> 0.21829408</td><td>-1.0991931 </td><td>-0.03247975</td><td>-0.5151724 </td><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td>0          </td><td>110200     </td></tr>\n",
       "\t<tr><td> 0.62874196</td><td>-0.7265431 </td><td> 1.61776810</td><td> 0.10024464</td><td> 0.24706505</td><td>-0.6573622 </td><td>-0.07763347</td><td>-0.4598522 </td><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td>0          </td><td>350900     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllll}\n",
       " longitude & latitude & housing\\_median\\_age & population & households & median\\_income & mean\\_bedrooms & mean\\_rooms & NEAR BAY & <1H OCEAN & INLAND & NEAR OCEAN & ISLAND & median\\_house\\_value\\\\\n",
       "\\hline\n",
       "\t  0.06473791 &  0.4485767  & -0.05081113 & -0.08342596 & -0.50882695 & -1.2394168  & -0.03648780 & -0.4145713  & 0           & 0           & 1           & 0           & 0           &  56700     \\\\\n",
       "\t -0.74882545 &  1.6471053  & -1.08374113 &  1.39212008 &  2.14071836 & -0.7358959  & -0.19291092 & -0.1004065  & 0           & 0           & 1           & 0           & 0           & 143400     \\\\\n",
       "\t  1.07295753 & -0.7218613  & -0.05081113 &  0.28656434 &  0.06136148 &  0.1404495  & -0.18700644 &  0.2732884  & 0           & 0           & 1           & 0           & 0           & 128300     \\\\\n",
       "\t -1.25293526 &  1.0759315  &  0.50538194 &  0.35897294 &  0.42492199 &  0.6344959  & -0.11581168 &  0.2741324  & 1           & 0           & 0           & 0           & 0           & 233200     \\\\\n",
       "\t  0.67865382 & -0.8061329  & -0.20972344 &  1.03802435 &  0.21829408 & -1.0991931  & -0.03247975 & -0.5151724  & 0           & 1           & 0           & 0           & 0           & 110200     \\\\\n",
       "\t  0.62874196 & -0.7265431  &  1.61776810 &  0.10024464 &  0.24706505 & -0.6573622  & -0.07763347 & -0.4598522  & 0           & 1           & 0           & 0           & 0           & 350900     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "longitude | latitude | housing_median_age | population | households | median_income | mean_bedrooms | mean_rooms | NEAR BAY | <1H OCEAN | INLAND | NEAR OCEAN | ISLAND | median_house_value | \n",
       "|---|---|---|---|---|---|\n",
       "|  0.06473791 |  0.4485767  | -0.05081113 | -0.08342596 | -0.50882695 | -1.2394168  | -0.03648780 | -0.4145713  | 0           | 0           | 1           | 0           | 0           |  56700      | \n",
       "| -0.74882545 |  1.6471053  | -1.08374113 |  1.39212008 |  2.14071836 | -0.7358959  | -0.19291092 | -0.1004065  | 0           | 0           | 1           | 0           | 0           | 143400      | \n",
       "|  1.07295753 | -0.7218613  | -0.05081113 |  0.28656434 |  0.06136148 |  0.1404495  | -0.18700644 |  0.2732884  | 0           | 0           | 1           | 0           | 0           | 128300      | \n",
       "| -1.25293526 |  1.0759315  |  0.50538194 |  0.35897294 |  0.42492199 |  0.6344959  | -0.11581168 |  0.2741324  | 1           | 0           | 0           | 0           | 0           | 233200      | \n",
       "|  0.67865382 | -0.8061329  | -0.20972344 |  1.03802435 |  0.21829408 | -1.0991931  | -0.03247975 | -0.5151724  | 0           | 1           | 0           | 0           | 0           | 110200      | \n",
       "|  0.62874196 | -0.7265431  |  1.61776810 |  0.10024464 |  0.24706505 | -0.6573622  | -0.07763347 | -0.4598522  | 0           | 1           | 0           | 0           | 0           | 350900      | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  longitude   latitude   housing_median_age population  households \n",
       "1  0.06473791  0.4485767 -0.05081113        -0.08342596 -0.50882695\n",
       "2 -0.74882545  1.6471053 -1.08374113         1.39212008  2.14071836\n",
       "3  1.07295753 -0.7218613 -0.05081113         0.28656434  0.06136148\n",
       "4 -1.25293526  1.0759315  0.50538194         0.35897294  0.42492199\n",
       "5  0.67865382 -0.8061329 -0.20972344         1.03802435  0.21829408\n",
       "6  0.62874196 -0.7265431  1.61776810         0.10024464  0.24706505\n",
       "  median_income mean_bedrooms mean_rooms NEAR BAY <1H OCEAN INLAND NEAR OCEAN\n",
       "1 -1.2394168    -0.03648780   -0.4145713 0        0         1      0         \n",
       "2 -0.7358959    -0.19291092   -0.1004065 0        0         1      0         \n",
       "3  0.1404495    -0.18700644    0.2732884 0        0         1      0         \n",
       "4  0.6344959    -0.11581168    0.2741324 1        0         0      0         \n",
       "5 -1.0991931    -0.03247975   -0.5151724 0        1         0      0         \n",
       "6 -0.6573622    -0.07763347   -0.4598522 0        1         0      0         \n",
       "  ISLAND median_house_value\n",
       "1 0       56700            \n",
       "2 0      143400            \n",
       "3 0      128300            \n",
       "4 0      233200            \n",
       "5 0      110200            \n",
       "6 0      350900            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "\n",
    "housing.tidy = read_csv('housing.csv')\n",
    "\n",
    "housing.tidy = housing.tidy %>% \n",
    "  mutate(total_bedrooms = ifelse(is.na(total_bedrooms), \n",
    "                                 median(total_bedrooms, na.rm = T),\n",
    "                                 total_bedrooms),\n",
    "         mean_bedrooms = total_bedrooms/households,\n",
    "         mean_rooms = total_rooms/households) %>%\n",
    "  select(-c(total_rooms, total_bedrooms))\n",
    "\n",
    "\n",
    "categories = unique(housing.tidy$ocean_proximity) # all categories\n",
    "\n",
    "cat_housing.tidy = categories %>% # compare the full vector against each category consecutively\n",
    "  lapply(function(x) as.numeric(housing.tidy$ocean_proximity == x)) %>% # convert to numeric\n",
    "  do.call(\"cbind\", .) %>% as_tibble() # clean up\n",
    "colnames(cat_housing.tidy) = categories # make nice column names\n",
    "\n",
    "cleaned_housing.tidy = housing.tidy %>% \n",
    "  select(-c(ocean_proximity, median_house_value)) %>%\n",
    "  scale() %>% as_tibble() %>%\n",
    "  bind_cols(cat_housing.tidy) %>%\n",
    "  add_column(median_house_value = housing.tidy$median_house_value)\n",
    "\n",
    "set.seed(19) # Set a random seed so that same sample can be reproduced in future runs\n",
    "\n",
    "sample = sample.int(n = nrow(cleaned_housing.tidy), size = floor(.8*nrow(cleaned_housing.tidy)), replace = F)\n",
    "train = cleaned_housing.tidy[sample, ] #just the samples\n",
    "test  = cleaned_housing.tidy[-sample, ] #everything but the samples\n",
    "      \n",
    "head(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# Random Forest Model\n",
    "########\n",
    "rf_model = randomForest(train_x, y = train_y , ntree = 500, importance = TRUE)\n",
    "\n",
    "names(rf_model) #these are all the different things you can call from the model.\n",
    "\n",
    "importance_dat = rf_model$importance\n",
    "importance_dat\n",
    "\n",
    "sorted_predictors = sort(importance_dat[,1], decreasing=TRUE)\n",
    "sorted_predictors\n",
    "\n",
    "oob_prediction = predict(rf_model) #leaving out a data source forces OOB predictions\n",
    "\n",
    "#you may have noticed that this is avaliable using the $mse in the model options.\n",
    "#but this way we learn stuff!\n",
    "train_mse = mean(as.numeric((oob_prediction - train_y)^2))\n",
    "oob_rmse = sqrt(train_mse)\n",
    "oob_rmse\n",
    "\n",
    "\n",
    "y_pred_rf = predict(rf_model , test_x)\n",
    "test_mse = mean(((y_pred_rf - test_y)^2))\n",
    "test_rmse = sqrt(test_mse)\n",
    "test_rmse # ~48620\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "######\n",
    "# XG Boost\n",
    "######\n",
    "# http://cran.fhcrc.org/web/packages/xgboost/vignettes/xgboost.pdf\n",
    "library(xgboost)\n",
    "\n",
    "#put into the xgb matrix format\n",
    "dtrain = xgb.DMatrix(data =  as.matrix(train_x), label = train_y )\n",
    "dtest = xgb.DMatrix(data =  as.matrix(test_x), label = test_y)\n",
    "\n",
    "# these are the datasets the rmse is evaluated for at each iteration\n",
    "watchlist = list(train=dtrain, test=dtest)\n",
    "\n",
    "# try 1 -off a set of paramaters I know work pretty well generally\n",
    "\n",
    "bst = xgb.train(data = dtrain, \n",
    "                max.depth = 8, \n",
    "                eta = 0.3, \n",
    "                nthread = 2, \n",
    "                nround = 1000, \n",
    "                watchlist = watchlist, \n",
    "                objective = \"reg:linear\", \n",
    "                early_stopping_rounds = 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try a 'slower learning' model. The up and down weights for each iteration are smaller\n",
    "# we also use more iterations\n",
    "\n",
    "bst_slow = xgb.train(data = dtrain, \n",
    "                        max.depth=6, \n",
    "                        eta = 0.01, \n",
    "                        nthread = 2, \n",
    "                        nround = 10000, \n",
    "                        watchlist = watchlist, \n",
    "                        objective = \"reg:linear\", \n",
    "                        early_stopping_rounds = 50)\n",
    "\n",
    "# note the best iteration is not the last iteration. \n",
    "XGBoost_importance = xgb.importance(feature_names = names(train_x), model = bst_slow)\n",
    "XGBoost_importance[1:10]\n",
    "\n",
    "# rmse: 45225.968750 # max.depth=5\n",
    "#last week: $48620 with random forest\n",
    "# an improvement of ~$3400 in average error. Wait! What we have done here is fit to the training set (leading to model overfit). Need to work with a validation set, then only at the end evaluate the model performance against the test set.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "####\n",
    "# Proper use - validation set\n",
    "####\n",
    "\n",
    "#validation set - Another subset of our data that is witheld from the training algorithm, but compared against at each iteration to see how\n",
    "\n",
    "#make validation set\n",
    "\n",
    "set.seed(19) # Set a random seed so that same sample can be reproduced in future runs\n",
    "\n",
    "sample = sample.int(n = nrow(train), size = floor(.8*nrow(train)), replace = F)\n",
    "\n",
    "train_t = train[sample, ] #just the samples\n",
    "valid  = train[-sample, ] #everything but the samples\n",
    "\n",
    "train_y = train_t[,'median_house_value']\n",
    "train_x = train_t[, names(train) !='median_house_value']\n",
    "\n",
    "valid_y = valid[,'median_house_value']\n",
    "valid_x = valid[, names(test) !='median_house_value']\n",
    "\n",
    "gb_train = xgb.DMatrix(data = as.matrix(train_x), label = train_y )\n",
    "gb_valid = xgb.DMatrix(data = as.matrix(valid_x), label = valid_y)\n",
    "\n",
    "# train xgb, evaluating against the validation\n",
    "watchlist = list(train = gb_train, valid = gb_valid)\n",
    "\n",
    "bst_slow = xgb.train(data= gb_train, \n",
    "                        max.depth = 10, \n",
    "                        eta = 0.01, \n",
    "                        nthread = 2, \n",
    "                        nround = 10000, \n",
    "                        watchlist = watchlist, \n",
    "                        objective = \"reg:linear\", \n",
    "                        early_stopping_rounds = 50)\n",
    "\n",
    "# error, need the matrix format\n",
    "y_hat = predict(bst_slow, test_x)\n",
    "\n",
    "# recall we ran the following to get the test data in the right format:\n",
    "# dtest = xgb.DMatrix(data =  as.matrix(test_x), label = test_y)\n",
    "# here I have it with the label taken off, just to remind us its external data xgb would ignore the label though during predictions\n",
    "dtest = xgb.DMatrix(data =  as.matrix(test_x))\n",
    "\n",
    "#test the model on truly external data\n",
    "\n",
    "y_hat_valid = predict(bst_slow, dtest)\n",
    "\n",
    "test_mse = mean(((y_hat_valid - test_y)^2))\n",
    "test_rmse = sqrt(test_mse)\n",
    "test_rmse \n",
    "# ~47507.09 This is higher then on the first run through, but we can be confident that the improved score is not due to overfit thanks to our use of a validation set! point out that this is evidence of how a lower rmse isn't necessarily better, as we now have more confidence in external predictions.2.3% improvement over a basic random forest... is it worth the effort? The answer to this question always depends on the purpose of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "###\n",
    "# Grid search first principles \n",
    "###\n",
    "\n",
    "max.depths = c(3, 5, 7, 9)\n",
    "etas = c(0.01, 0.001, 0.0001)\n",
    "\n",
    "best_params = 0\n",
    "best_score = 0\n",
    "\n",
    "count = 1\n",
    "for( depth in max.depths ){\n",
    "    for( num in etas){\n",
    "\n",
    "        bst_grid = xgb.train(data = gb_train, \n",
    "                                max.depth = depth, \n",
    "                                eta=num, \n",
    "                                nthread = 2, \n",
    "                                nround = 10000, \n",
    "                                watchlist = watchlist, \n",
    "                                objective = \"reg:linear\", \n",
    "                                early_stopping_rounds = 50, \n",
    "                                verbose=0)\n",
    "\n",
    "        if(count == 1){\n",
    "            best_params = bst_grid$params\n",
    "            best_score = bst_grid$best_score\n",
    "            count = count + 1\n",
    "            }\n",
    "        else if( bst_grid$best_score < best_score){\n",
    "            best_params = bst_grid$params\n",
    "            best_score = bst_grid$best_score\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "best_params\n",
    "best_score\n",
    "#valid-rmse: 47033.28\n",
    "\n",
    "# max_depth of 9, eta of 0.01\n",
    "bst_tuned = xgb.train( data = gb_train, \n",
    "                        max.depth = 9, \n",
    "                        eta = 0.01, \n",
    "                        nthread = 2, \n",
    "                        nround = 10000, \n",
    "                        watchlist = watchlist, \n",
    "                        objective = \"reg:linear\", \n",
    "                        early_stopping_rounds = 50)\n",
    "\n",
    "y_hat_xgb_grid = predict(bst_tuned, dtest)\n",
    "\n",
    "test_mse = mean(((y_hat_xgb_grid - test_y)^2))\n",
    "test_rmse = sqrt(test_mse)\n",
    "test_rmse # test-rmse: 46675\n",
    "# By tuning the hyperparamaters we have moved to a 4% improvement over random forest\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#######\n",
    "# tweak the hyperparamaters using a grid search\n",
    "# The caret package (short for classification and regression training)\n",
    "######\n",
    "\n",
    "library(caret) \n",
    "\n",
    "# look up the model we are running to see the paramaters\n",
    "modelLookup(\"xgbLinear\")\n",
    " \n",
    "# set up all the pairwise combinations\n",
    "\n",
    "xgb_grid_1 = expand.grid(nrounds = c(1000,2000,3000,4000) ,\n",
    "                            eta = c(0.01, 0.001, 0.0001),\n",
    "                            lambda = 1,\n",
    "                            alpha = 0)\n",
    "xgb_grid_1\n",
    "\n",
    "\n",
    "#here we do one better then a validation set, we use cross validation to \n",
    "#expand the amount of info we have!\n",
    "xgb_trcontrol_1 = trainControl(method = \"cv\",\n",
    "                                number = 5,\n",
    "                                verboseIter = TRUE,\n",
    "                                returnData = FALSE,\n",
    "                                returnResamp = \"all\", \n",
    "                                allowParallel = TRUE)\n",
    "\n",
    "\n",
    "######\n",
    "#below a grid-search, cross-validation xgboost model in caret\n",
    "######\n",
    "# train the model for each parameter combination in the grid, using CV to evaluate on multiple folds. Make sure your laptop is plugged in or RIP battery.\n",
    "\n",
    "# note how this is now the caret train function\n",
    "?train\n",
    "\n",
    "xgb_train_1 = train(x = as.matrix(train_x),\n",
    "\t\t\t\t\ty = train_y,\n",
    "\t\t\t\t\ttrControl = xgb_trcontrol_1,\n",
    "\t\t\t\t\ttuneGrid = xgb_grid_1,\n",
    "\t\t\t\t\tmethod = \"xgbLinear\",\n",
    "\t\t\t\t\tmax.depth = 5)\n",
    "\n",
    "names(xgb_train_1)\n",
    "xgb_train_1$bestTune\n",
    "xgb_train_1$method\n",
    "summary(xgb_train_1)\n",
    "\n",
    "\n",
    "#alternatively, you can 'narrow in' on the best paramaters by taking a range of options around the best values found and seeing if high resolution tweaks can provide even further improvements.\n",
    "\n",
    "xgb_cv_yhat = predict(xgb_train_1 , as.matrix(test_x))\n",
    "\n",
    "\n",
    "test_mse = mean(((xgb_cv_yhat - test_y)^2))\n",
    "test_rmse = sqrt(test_mse)\n",
    "test_rmse # 46641... pretty close to the 'by hand' grid search!\n",
    "\n",
    "#Cam's hypothesis - not using 'early stopping rounds' here so the model isn't cutting out at the exact best point. re-running this with a validation setup as opposed to a cv setup would allow us to implement a grid search efficiently and wind up with the best hyperparamaters. I shall leave this as a follow up exercise for the curious.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "########\n",
    "# Ensemble the models together, \n",
    "# strategy for when accuracy is more important then knowing the best predictors\n",
    "########\n",
    "\n",
    "\n",
    "y_pred_rf #random forest\n",
    "y_hat_valid #xgBoost with validation\n",
    "y_hat_xgb_grid #xgBoost grid search\n",
    "xgb_cv_yhat #xgBoost caret cross validation\n",
    "\n",
    "length(y_hat_xgb_grid)\n",
    "\n",
    "\n",
    "blend_pred = (y_hat * .25) + (y_pred_rf * .25) + (xgb_cv_yhat * .25) + (y_hat_xgb_grid * .25)\n",
    "length(blend_pred)\n",
    "\n",
    "length(blend_pred) == length(y_hat_xgb_grid)\n",
    "\n",
    "blend_test_mse = mean(((blend_pred - test_y)^2))\n",
    "blend_test_rmse = sqrt(blend_test_mse)\n",
    "blend_test_rmse # 45205 by averaging just 4 predictors we have dropped the rmse a few percent lower then the best scoring of the 4 models. This does come at a cost though, we now can't make accurate inferrences about the best predictors!\n",
    "\n",
    "#next step - you can grid search the weights of the ensemble to try and drop the rmse further!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
